# -*- coding: utf-8 -*-
"""
Created on Fri Jun 20 09:02:35 2025

@author: hiroy

Enhanced Raman Spectrum Analysis Tool with HuggingFace Mistral and RAG functionality
"""

import time
import numpy as np
import pandas as pd
import seaborn as sns
import streamlit as st
import scipy.signal as signal
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.signal import savgol_filter, find_peaks, peak_prominences
from scipy.sparse.linalg import spsolve
from scipy.sparse import csc_matrix, eye, diags

from streamlit_plotly_events import plotly_events

# Additional imports for LLM and RAG functionality
import os
import glob
import PyPDF2
import docx
from datetime import datetime
from typing import List, Dict, Optional
from sentence_transformers import SentenceTransformer
import faiss
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Set matplotlib style
plt.style.use('default')
sns.set_palette("husl")

# RAGæ©Ÿèƒ½ã®ã‚¯ãƒ©ã‚¹å®šç¾©
class RamanRAGSystem:
    def __init__(self, embedding_model_name='all-MiniLM-L6-v2'):
        """
        RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        
        Args:
            embedding_model_name: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
        """
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.vector_db = None
        self.documents = []
        self.document_metadata = []
        self.embedding_dim = None
        
    def extract_text_from_file(self, file_path: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            file_ext = os.path.splitext(file_path)[1].lower()
            
            if file_ext == '.pdf':
                return self._extract_from_pdf(file_path)
            elif file_ext == '.docx':
                return self._extract_from_docx(file_path)
            elif file_ext == '.txt':
                return self._extract_from_txt(file_path)
            else:
                return ""
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _extract_from_pdf(self, file_path: str) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
        except Exception as e:
            st.error(f"PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return text
    
    def _extract_from_docx(self, file_path: str) -> str:
        """DOCXã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            doc = docx.Document(file_path)
            text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
            return text
        except Exception as e:
            st.error(f"DOCXèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _extract_from_txt(self, file_path: str) -> str:
        """TXTã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='shift_jis') as file:
                    return file.read()
            except Exception as e:
                st.error(f"TXTèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return ""
        except Exception as e:
            st.error(f"TXTèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        
        Args:
            text: åˆ†å‰²ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            overlap: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º
            
        Returns:
            ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip():
                chunks.append(chunk)
                
        return chunks
    
    def build_vector_database(self, folder_path: str):
        """
        æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰è«–æ–‡ã‚’èª­ã¿è¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰
        
        Args:
            folder_path: è«–æ–‡ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        """
        if not os.path.exists(folder_path):
            st.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return
        
        # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
        file_patterns = ['*.pdf', '*.docx', '*.txt']
        files = []
        for pattern in file_patterns:
            files.extend(glob.glob(os.path.join(folder_path, pattern)))
        
        if not files:
            st.warning("æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«è«–æ–‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        st.info(f"è«–æ–‡ãƒ•ã‚¡ã‚¤ãƒ« {len(files)} ä»¶ã‚’å‡¦ç†ä¸­...")
        
        all_chunks = []
        all_metadata = []
        
        progress_bar = st.progress(0)
        
        for i, file_path in enumerate(files):
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            text = self.extract_text_from_file(file_path)
            
            if text:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                chunks = self.chunk_text(text)
                
                for chunk in chunks:
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'filename': os.path.basename(file_path),
                        'filepath': file_path,
                        'chunk_text': chunk[:100] + "..." if len(chunk) > 100 else chunk
                    })
            
            progress_bar.progress((i + 1) / len(files))
        
        if not all_chunks:
            st.error("å‡¦ç†å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        st.info("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­...")
        embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=True)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
        self.embedding_dim = embeddings.shape[1]
        self.vector_db = faiss.IndexFlatIP(self.embedding_dim)  # Inner product for similarity
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–
        faiss.normalize_L2(embeddings.astype(np.float32))
        self.vector_db.add(embeddings.astype(np.float32))
        
        self.documents = all_chunks
        self.document_metadata = all_metadata
        
        st.success(f"ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰å®Œäº†: {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")
    
    def search_relevant_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            top_k: å–å¾—ã™ã‚‹ä¸Šä½æ–‡æ›¸æ•°
            
        Returns:
            é–¢é€£æ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
        """
        if self.vector_db is None:
            return []
        
        # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding.astype(np.float32))
        
        # é¡ä¼¼æ–‡æ›¸ã‚’æ¤œç´¢
        scores, indices = self.vector_db.search(query_embedding.astype(np.float32), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                results.append({
                    'text': self.documents[idx],
                    'metadata': self.document_metadata[idx],
                    'similarity_score': float(score)
                })
        
        return results

class HuggingFaceMistralLLM:
    """
    Hugging Face Mistralãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, model_name="mistralai/Mistral-7B-Instruct-v0.2"):
        """
        Mistralãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        
        Args:
            model_name: ä½¿ç”¨ã™ã‚‹Mistralãƒ¢ãƒ‡ãƒ«å
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self._load_model()
    
    def _load_model(self):
        """
        ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
        """
        try:
            with st.spinner(f"Mistralãƒ¢ãƒ‡ãƒ« ({self.model_name}) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                # ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼ˆGPUåˆ©ç”¨å¯èƒ½ãªã‚‰ä½¿ç”¨ï¼‰
                device = 0 if torch.cuda.is_available() else -1
                
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
                self.pipeline = pipeline(
                    "text-generation",
                    model=self.model_name,
                    device=device,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    do_sample=True,
                    temperature=0.3,
                    max_new_tokens=1024,
                    return_full_text=False
                )
                
                st.success(f"âœ… Mistralãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                
        except Exception as e:
            st.error(f"Mistralãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            self.pipeline = None
    
    def generate_response(self, prompt: str, max_tokens: int = 1024) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if self.pipeline is None:
            return "âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒæ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        
        try:
            # Mistralç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            formatted_prompt = f"<s>[INST] {prompt} [/INST]"
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            response = self.pipeline(
                formatted_prompt,
                max_new_tokens=max_tokens,
                temperature=0.3,
                do_sample=True,
                pad_token_id=self.pipeline.tokenizer.eos_token_id
            )
            
            return response[0]['generated_text'].strip()
            
        except Exception as e:
            return f"âš ï¸ å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
    
    def generate_stream_response(self, prompt: str, max_tokens: int = 1024):
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å¿œç­”ã‚’ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Yields:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®æ–­ç‰‡
        """
        # æ³¨: æœ¬æ¥ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯è¤‡é›‘ãªãŸã‚ã€ã“ã“ã§ã¯å…¨æ–‡ç”Ÿæˆå¾Œã«åˆ†å‰²ã—ã¦è¿”ã™
        full_response = self.generate_response(prompt, max_tokens)
        
        # æ–‡å­—å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é¢¨ã«è¿”ã™
        for i in range(0, len(full_response), 10):
            chunk = full_response[i:i+10]
            yield chunk
            time.sleep(0.05)  # è¦–è¦šåŠ¹æœã®ãŸã‚ã®é…å»¶

class RamanSpectrumAnalyzer:
    def generate_analysis_prompt(
        self,
        peak_data: List[Dict],
        relevant_docs: List[Dict],
        user_hint: Optional[str] = None
    ) -> str:
        """
        ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ

        Args:
            peak_data: ãƒ”ãƒ¼ã‚¯æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            relevant_docs: é–¢é€£æ–‡çŒ®æƒ…å ±ï¼ˆRAGçµæœï¼‰
            user_hint: ãƒ¦ãƒ¼ã‚¶ãƒ¼è£œè¶³ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰

        Returns:
            LLMç”¨è§£æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """

        def format_peaks(peaks: List[Dict]) -> str:
            header = "ã€æ¤œå‡ºãƒ”ãƒ¼ã‚¯ä¸€è¦§ã€‘"
            lines = [
                f"{i+1}. æ³¢æ•°: {p.get('wavenumber', 0):.1f} cmâ»Â¹, "
                f"å¼·åº¦: {p.get('intensity', 0):.3f}, "
                f"prominence: {p.get('prominence', 0):.3f}, "
                f"ç¨®é¡: {'è‡ªå‹•æ¤œå‡º' if p.get('type') == 'auto' else 'æ‰‹å‹•è¿½åŠ '}"
                for i, p in enumerate(peaks)
            ]
            return "\n".join([header] + lines)

        def format_reference_excerpts(docs: List[Dict]) -> str:
            header = "ã€å¼•ç”¨æ–‡çŒ®ã®æŠœç²‹ã¨è¦ç´„ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                title = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                page = doc.get("metadata", {}).get("page")
                summary = doc.get("page_content", "").strip()
                lines.append(f"\n--- å¼•ç”¨{i} ---")
                lines.append(f"å‡ºå…¸ãƒ•ã‚¡ã‚¤ãƒ«: {title}")
                if page is not None:
                    lines.append(f"ãƒšãƒ¼ã‚¸ç•ªå·: {page}")
                lines.append(f"æŠœç²‹å†…å®¹:\n{summary}")
            return "\n".join([header] + lines)

        def format_doc_summaries(docs: List[Dict], preview_length: int = 300) -> str:
            header = "ã€æ–‡çŒ®ã®æ¦‚è¦ï¼ˆé¡ä¼¼åº¦ä»˜ãï¼‰ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                filename = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                similarity = doc.get("similarity_score", 0.0)
                text = doc.get("text") or doc.get("page_content") or ""
                lines.append(
                    f"æ–‡çŒ®{i} (é¡ä¼¼åº¦: {similarity:.3f})\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\n"
                    f"å†’é ­æŠœç²‹: {text.strip()[:preview_length]}...\n"
                )
            return "\n".join([header] + lines)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã®æ§‹ç¯‰
        sections = [
            "ä»¥ä¸‹ã¯ã€ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã§æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æƒ…å ±ã§ã™ã€‚",
            "ã“ã‚Œã‚‰ã®ãƒ”ãƒ¼ã‚¯ã«åŸºã¥ãã€è©¦æ–™ã®æˆåˆ†ã‚„ç‰¹å¾´ã«ã¤ã„ã¦æ¨å®šã—ã¦ãã ã•ã„ã€‚",
            "ãªãŠã€æ–‡çŒ®ã¨ã®æ¯”è¼ƒã«ãŠã„ã¦ã¯ãƒ”ãƒ¼ã‚¯ä½ç½®ãŒÂ±5cmâ»Â¹ç¨‹åº¦ãšã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãã®ãŸã‚ã€Â±5cmâ»Â¹ä»¥å†…ã®å·®ã§ã‚ã‚Œã°ä¸€è‡´ã¨ã¿ãªã—ã¦è§£æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n"
        ]

        if user_hint:
            sections.append(f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹è£œè¶³æƒ…å ±ã€‘\n{user_hint}\n")

        if peak_data:
            sections.append(format_peaks(peak_data))
        if relevant_docs:
            sections.append(format_reference_excerpts(relevant_docs))
            sections.append(format_doc_summaries(relevant_docs))

        sections.append(
            "ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€è©¦æ–™ã«å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹åŒ–åˆç‰©ã‚„ç‰©è³ªæ§‹é€ ã€ç‰¹å¾´ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n"
            "å‡ºåŠ›ã¯æ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n"
            "## è§£æã®è¦³ç‚¹:\n"
            "1. å„ãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„å¸°å±ã¨ãã®æ ¹æ‹ \n"
            "2. è©¦æ–™ã®å¯èƒ½ãªçµ„æˆã‚„æ§‹é€ \n"
            "3. æ–‡çŒ®æƒ…å ±ã¨ã®æ¯”è¼ƒãƒ»å¯¾ç…§\n\n"
            "è©³ç´°ã§ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸè€ƒå¯Ÿã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )

        return "\n".join(sections)

# æ—¢å­˜ã®é–¢æ•°ç¾¤ï¼ˆå¤‰æ›´ãªã—ï¼‰
def create_features_labels(spectra, window_size=10):
    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®é…åˆ—ã‚’åˆæœŸåŒ–
    X = []
    y = []
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®é•·ã•
    n_points = len(spectra)
    # äººæ‰‹ã«ã‚ˆã‚‹ãƒ”ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«ã€ã¾ãŸã¯è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«é…ç½®
    peak_labels = np.zeros(n_points)

    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®æŠ½å‡º
    for i in range(window_size, n_points - window_size):
        # å‰å¾Œã®çª“ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨
        features = spectra[i-window_size:i+window_size+1]
        X.append(features)
        y.append(peak_labels[i])

    return np.array(X), np.array(y)

def find_index(rs_array,  rs_focused):
    '''
    Convert the index of the proximate wavenumber by finding the absolute 
    minimum value of (rs_array - rs_focused)
    
    input
        rs_array: Raman wavenumber
        rs_focused: Index
    output
        index
    '''

    diff = [abs(element - rs_focused) for element in rs_array]
    index = np.argmin(diff)
    return index

def WhittakerSmooth(x,w,lambda_,differences=1):
    '''
    Penalized least squares algorithm for background fitting
    
    input
        x: input data (i.e. chromatogram of spectrum)
        w: binary masks (value of the mask is zero if a point belongs to peaks and one otherwise)
        lambda_: parameter that can be adjusted by user. The larger lambda is,  the smoother the resulting background
        differences: integer indicating the order of the difference of penalties
    
    output
        the fitted background vector
    '''
    X=np.array(x, dtype=np.float64)
    m=X.size
    E=eye(m,format='csc')
    for i in range(differences):
        E=E[1:]-E[:-1] # numpy.diff() does not work with sparse matrix. This is a workaround.
    W=diags(w,0,shape=(m,m))
    A=csc_matrix(W+(lambda_*E.T*E))
    B=csc_matrix(W*X.T).toarray().flatten()
    background=spsolve(A,B)
    return np.array(background)

def airPLS(x, dssn_th, lambda_, porder, itermax):
    '''
    Adaptive iteratively reweighted penalized least squares for baseline fitting
    
    input
        x: input data (i.e. chromatogram or spectrum)
        lambda_: parameter that can be adjusted by user. The larger lambda is, the smoother the resulting background, z
        porder: adaptive iteratively reweighted penalized least squares for baseline fitting
    
    output
        the fitted background vector
    '''
    # ãƒã‚¤ãƒŠã‚¹å€¤ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
    min_value = np.min(x)
    offset = 0
    if min_value < 0:
        offset = abs(min_value) + 1  # æœ€å°å€¤ã‚’1ã«ã™ã‚‹ãŸã‚ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        x = x + offset  # å…¨ä½“ã‚’ã‚·ãƒ•ãƒˆ
    
    m = x.shape[0]
    w = np.ones(m, dtype=np.float64)  # æ˜ç¤ºçš„ã«å‹ã‚’æŒ‡å®š
    x = np.asarray(x, dtype=np.float64)  # xã‚‚æ˜ç¤ºçš„ã«å‹ã‚’æŒ‡å®š
    
    for i in range(1, itermax + 1):
        z = WhittakerSmooth(x, w, lambda_, porder)
        d = x - z
        dssn = np.abs(d[d < 0].sum())
        
        # dssn ãŒã‚¼ãƒ­ã¾ãŸã¯éå¸¸ã«å°ã•ã„å ´åˆã‚’å›é¿
        if dssn < 1e-10:
            dssn = 1e-10
        
        # åæŸåˆ¤å®š
        if (dssn < dssn_th * (np.abs(x)).sum()) or (i == itermax):
            if i == itermax:
                print('WARNING: max iteration reached!')
            break
        
        # é‡ã¿ã®æ›´æ–°
        w[d >= 0] = 0  # d > 0 ã¯ãƒ”ãƒ¼ã‚¯ã®ä¸€éƒ¨ã¨ã—ã¦é‡ã¿ã‚’ç„¡è¦–
        w[d < 0] = np.exp(i * np.abs(d[d < 0]) / dssn)
        
        # å¢ƒç•Œæ¡ä»¶ã®èª¿æ•´
        if d[d < 0].size > 0:
            w[0] = np.exp(i * np.abs(d[d < 0]).max() / dssn)
        else:
            w[0] = 1.0  # é©åˆ‡ãªåˆæœŸå€¤
        
        w[-1] = w[0]

    return z

def detect_file_type(data):
    """
    Determine the structure of the input data.
    """
    try:
        if data.columns[0].split(':')[0] == "# Laser Wavelength":
            return "ramaneye_new"
        elif data.columns[0] == "WaveNumber":
            return "ramaneye_old"
        elif data.columns[0] == "Pixels":
            return "eagle"
        elif data.columns[0] == "ENLIGHTEN Version":
            return "wasatch"
        return "unknown"
    except:
        return "unknown"

def read_csv_file(uploaded_file, file_extension):
    """
    Read a CSV or TXT file into a DataFrame based on file extension.
    """
    try:
        uploaded_file.seek(0)
        if file_extension == "csv":
            data = pd.read_csv(uploaded_file, sep=',', header=0, index_col=None, on_bad_lines='skip')
        else:
            data = pd.read_csv(uploaded_file, sep='\t', header=0, index_col=None, on_bad_lines='skip')
        return data
    except UnicodeDecodeError:
        uploaded_file.seek(0)
        if file_extension == "csv":
            data = pd.read_csv(uploaded_file, sep=',', encoding='shift_jis', header=0, index_col=None, on_bad_lines='skip')
        else:
            data = pd.read_csv(uploaded_file, sep='\t', encoding='shift_jis', header=0, index_col=None, on_bad_lines='skip')
        return data
    except Exception as e:
        st.error(f"Error reading file: {e}")
        return None

def calculate_peak_width(spectrum, peak_idx, wavenum):
    """
    ãƒ”ãƒ¼ã‚¯ã®åŠå€¤å¹…ï¼ˆFWHMï¼‰ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    
    Parameters:
    spectrum (ndarray): ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
    peak_idx (int): ãƒ”ãƒ¼ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    wavenum (ndarray): æ³¢æ•°ãƒ‡ãƒ¼ã‚¿
    
    Returns:
    fwhm (float): åŠå€¤å¹… (cmâ»Â¹)
    """
    if peak_idx <= 0 or peak_idx >= len(spectrum) - 1:
        return 0.0
    
    peak_intensity = spectrum[peak_idx]
    half_max = peak_intensity / 2.0
    
    # ãƒ”ãƒ¼ã‚¯ã‹ã‚‰å·¦å´ã«å‘ã‹ã£ã¦åŠå€¤ç‚¹ã‚’æ¢ã™
    left_idx = peak_idx
    while left_idx > 0 and spectrum[left_idx] > half_max:
        left_idx -= 1
    
    # ç·šå½¢è£œé–“ã§æ­£ç¢ºãªåŠå€¤ç‚¹ã‚’æ±‚ã‚ã‚‹
    if left_idx < peak_idx and spectrum[left_idx] <= half_max < spectrum[left_idx + 1]:
        # ç·šå½¢è£œé–“
        ratio = (half_max - spectrum[left_idx]) / (spectrum[left_idx + 1] - spectrum[left_idx])
        left_wavenum = wavenum[left_idx] + ratio * (wavenum[left_idx + 1] - wavenum[left_idx])
    else:
        left_wavenum = wavenum[left_idx] if left_idx >= 0 else wavenum[0]
    
    # ãƒ”ãƒ¼ã‚¯ã‹ã‚‰å³å´ã«å‘ã‹ã£ã¦åŠå€¤ç‚¹ã‚’æ¢ã™
    right_idx = peak_idx
    while right_idx < len(spectrum) - 1 and spectrum[right_idx] > half_max:
        right_idx += 1
    
    # ç·šå½¢è£œé–“ã§æ­£ç¢ºãªåŠå€¤ç‚¹ã‚’æ±‚ã‚ã‚‹
    if right_idx > peak_idx and spectrum[right_idx] <= half_max < spectrum[right_idx - 1]:
        # ç·šå½¢è£œé–“
        ratio = (half_max - spectrum[right_idx]) / (spectrum[right_idx - 1] - spectrum[right_idx])
        right_wavenum = wavenum[right_idx] + ratio * (wavenum[right_idx - 1] - wavenum[right_idx])
    else:
        right_wavenum = wavenum[right_idx] if right_idx < len(wavenum) else wavenum[-1]
    
    # åŠå€¤å¹…ã‚’è¨ˆç®—
    fwhm = abs(right_wavenum - left_wavenum)
    return fwhm

def remove_outliers_and_interpolate(spectrum, window_size=10, threshold_factor=3):
    """
    ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ã‚¯ï¼ˆå¤–ã‚Œå€¤ï¼‰ã‚’æ¤œå‡ºã—ã€è£œå®Œã™ã‚‹é–¢æ•°
    ã‚¹ãƒ‘ã‚¤ã‚¯ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®æ¨™æº–åå·®ãŒä¸€å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹å ´åˆã«æ¤œå‡ºã•ã‚Œã‚‹
    
    input:
        spectrum: numpy array, ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«
        window_size: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯20ï¼‰
        threshold_factor: æ¨™æº–åå·®ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯5å€ï¼‰
    
    output:
        cleaned_spectrum: numpy array, ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å–ã‚Šé™¤ãè£œå®Œã—ãŸã‚¹ãƒšã‚¯ãƒˆãƒ«
    """
    spectrum_len = len(spectrum)
    cleaned_spectrum = spectrum.copy()
    
    for i in range(spectrum_len):
        # ç«¯ç‚¹ã§ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒè¶³ã‚Šãªã„ã®ã§ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’èª¿æ•´
        left_idx = max(i - window_size, 0)
        right_idx = min(i + window_size + 1, spectrum_len)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        window = spectrum[left_idx:right_idx]
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ä¸­å¤®å€¤ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
        window_median = np.median(window)
        window_std = np.std(window)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®å€¤ãŒæ¨™æº–åå·®ã®é–¾å€¤ã‚’è¶…ãˆã‚‹ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¤œå‡º
        if abs(spectrum[i] - window_median) > threshold_factor * window_std:
            # ã‚¹ãƒ‘ã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®å€¤ã‚’ä¸¡éš£ã®ä¸­å¤®å€¤ã§è£œå®Œ
            if i > 0 and i < spectrum_len - 1:  # ä¸¡éš£ã®å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                cleaned_spectrum[i] = (spectrum[i - 1] + spectrum[i + 1]) / 2
            elif i == 0:  # å·¦ç«¯ã®å ´åˆ
                cleaned_spectrum[i] = spectrum[i + 1]
            elif i == spectrum_len - 1:  # å³ç«¯ã®å ´åˆ
                cleaned_spectrum[i] = spectrum[i - 1] 
    return cleaned_spectrum
    
# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒé–¢æ•°
def optimize_thresholds_via_gridsearch(
    wavenum, spectrum, second_derivative,
    manual_add_peaks, manual_exclude_indices,
    current_prom_thres, current_deriv_thres,
    detected_original_peaks=None,
    resolution=40
):
    best_score = -np.inf
    best_prom_thres = current_prom_thres
    best_deriv_thres = current_deriv_thres

    # å€åŠåˆ†ã®ç¯„å›²ã§ãƒ­ã‚°ã‚¹ã‚±ãƒ¼ãƒ«æ¤œç´¢
    prom_range = np.linspace(current_prom_thres / 2, current_prom_thres * 2, resolution)
    deriv_range = np.linspace(current_deriv_thres / 2, current_deriv_thres * 2, resolution)

    for prom_thres in prom_range:
        for deriv_thres in deriv_range:
            peaks, _ = find_peaks(-second_derivative, height=deriv_thres)
            prominences = peak_prominences(-second_derivative, peaks)[0]
            mask = prominences > prom_thres
            final_peaks = set(peaks[mask])

            score = 0

            # æ‰‹å‹•è¿½åŠ ãŒå«ã¾ã‚Œã¦ã„ã‚Œã° +1
            for x, _ in manual_add_peaks:
                idx = np.argmin(np.abs(wavenum - x))
                if idx in final_peaks:
                    score += 1

            # é™¤å¤–å¯¾è±¡ãŒå«ã¾ã‚Œã¦ã„ã‚Œã° -1
            for idx in manual_exclude_indices:
                if idx in final_peaks:
                    score -= 1

            # è‡ªå‹•ãƒ”ãƒ¼ã‚¯ã®é€¸è„±ãƒšãƒŠãƒ«ãƒ†ã‚£
            if len(detected_original_peaks) > 0:
                for idx in final_peaks:
                    if idx not in detected_original_peaks:
                        score -= 1  # æ–°ãŸã«ç¾ã‚ŒãŸä½™åˆ†ãªãƒ”ãƒ¼ã‚¯
                for idx in detected_original_peaks:
                    if idx not in final_peaks:
                        score -= 1  # å…ƒã®ãƒ”ãƒ¼ã‚¯ãŒæ¶ˆãˆãŸ

            # ã‚¹ã‚³ã‚¢æœ€å¤§ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
            if score > best_score:
                best_score = score
                best_prom_thres = prom_thres
                best_deriv_thres = deriv_thres

    return {
        "prominence_threshold": best_prom_thres,
        "second_deriv_threshold": best_deriv_thres,
        "score": best_score
    }

def spectrum_analysis_mode():
    st.header("ğŸ“Š ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ")
    
    # --- LLM/RAGè¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
    st.sidebar.subheader("ğŸ¤– AIè§£æè¨­å®š")
    
    # Mistralãƒ¢ãƒ‡ãƒ«é¸æŠ
    model_options = [
        "mistralai/Mistral-7B-Instruct-v0.2",
        "mistralai/Mistral-7B-Instruct-v0.1",
        "mistralai/Mixtral-8x7B-Instruct-v0.1"
    ]
    
    selected_model = st.sidebar.selectbox(
        "ğŸ§  ä½¿ç”¨ã™ã‚‹Mistralãƒ¢ãƒ‡ãƒ«",
        model_options,
        index=0,
        help="ä½¿ç”¨ã™ã‚‹Hugging Face Mistralãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰
    uploaded_files = st.sidebar.file_uploader(
        "ğŸ“„ æ–‡çŒ®PDFã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
        type=["pdf"],
        accept_multiple_files=True
    )
    
    # ä¸€æ™‚ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    TEMP_DIR = "./tmp_uploads"
    os.makedirs(TEMP_DIR, exist_ok=True)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã¨MistralLLMã®åˆæœŸåŒ–
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = RamanRAGSystem()
        st.session_state.rag_db_built = False
    
    if 'mistral_llm' not in st.session_state:
        st.session_state.mistral_llm = None
        st.session_state.current_model = None
        
    # ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ã®å†åˆæœŸåŒ–
    if st.session_state.current_model != selected_model:
        st.session_state.mistral_llm = None
        st.session_state.current_model = selected_model
        
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ§‹ç¯‰
    if st.sidebar.button("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"):
        if not uploaded_files:
            st.sidebar.warning("æ–‡çŒ®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("è«–æ–‡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­..."):
                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
                for uploaded_file in uploaded_files:
                    save_path = os.path.join(TEMP_DIR, uploaded_file.name)
                    with open(save_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())

                # ä¿å­˜ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ã£ã¦ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰
                st.session_state.rag_system.build_vector_database(TEMP_DIR)
                st.session_state.rag_db_built = True
                st.sidebar.success(f"âœ… {len(uploaded_files)} ä»¶ã®PDFã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹è¡¨ç¤º
    if st.session_state.rag_db_built:
        st.sidebar.success("âœ… è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰æ¸ˆã¿")
    else:
        st.sidebar.info("â„¹ï¸ è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœªæ§‹ç¯‰")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è£œè¶³æŒ‡ç¤ºæ¬„ã‚’è¿½åŠ 
    user_hint = st.sidebar.text_area(
        "ğŸ§ª AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰",
        placeholder="ä¾‹ï¼šã“ã®è©¦æ–™ã¯ãƒãƒªã‚¨ãƒãƒ¬ãƒ³ç³»é«˜åˆ†å­ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€ãªã©"
    )
    
    st.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ")

    # --- äº‹å‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
    pre_start_wavenum = 400
    pre_end_wavenum = 2000
    
    # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–ï¼ˆUIè¡¨ç¤ºã‚ˆã‚Šã‚‚å‰ï¼ï¼‰ ---
    for key, default in {
        "prominence_threshold": 100,
        "second_deriv_threshold": 100,
        "savgol_wsize": 5,
        "spectrum_type_select": "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤",
        "second_deriv_smooth": 5,
        "manual_peak_keys": []
    }.items():
        if key not in st.session_state:
            st.session_state[key] = default
    # --- UIãƒ‘ãƒãƒ«ï¼ˆSidebarï¼‰ ---
    start_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆé–‹å§‹ï¼‰:", -200, 4800, value=pre_start_wavenum, step=100)
    end_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆçµ‚äº†ï¼‰:", -200, 4800, value=pre_end_wavenum, step=100)
    dssn_th = st.sidebar.number_input("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼:", 1, 10000, value=1000, step=1) / 1e7
    
    savgol_wsize = st.sidebar.number_input(
        "ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º:", 3, 101, 
        value=st.session_state["savgol_wsize"], step=2, key="savgol_wsize"
    )
    
    st.sidebar.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºè¨­å®š")
    
    spectrum_type = st.sidebar.selectbox(
        "è§£æã‚¹ãƒšã‚¯ãƒˆãƒ«:", ["ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤", "ç§»å‹•å¹³å‡å¾Œ"], 
        index=0, key="spectrum_type_select"
    )
    
    second_deriv_smooth = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–:", 3, 35,
        value=st.session_state["second_deriv_smooth"],
        step=2, key="second_deriv_smooth"
    )
    
    # ä¸€æ™‚çš„ãªã‚»ãƒƒã‚·ãƒ§ãƒ³å¤‰æ•°ãŒå­˜åœ¨ã™ã‚‹ãªã‚‰ãã¡ã‚‰ã‚’ä½¿ã†ï¼ˆãªã‘ã‚Œã°é€šå¸¸ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å€¤ï¼‰
    prom_default = float(st.session_state.get("prominence_threshold_temp", st.session_state.get("prominence_threshold", 100.0)))
    second_default = float(st.session_state.get("second_deriv_threshold_temp", st.session_state.get("second_deriv_threshold", 100.0)))
    
    second_deriv_threshold = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†é–¾å€¤:",
        min_value=0.0,
        max_value=1000.0,
        value=second_default,
        step=10.0,
        key="second_deriv_threshold"
    )
    
    peak_prominence_threshold = st.sidebar.number_input(
        "ãƒ”ãƒ¼ã‚¯Prominenceé–¾å€¤:",
        min_value=0.0,
        max_value=1000.0,
        value=prom_default,
        step=10.0,
        key="prominence_threshold"
    )

    # --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆ1ç®‡æ‰€ã«çµ±ä¸€ï¼‰ ---
    uploaded_spectrum_files = st.file_uploader("ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", accept_multiple_files=True, key="spectrum_file_uploader")
    
    # --- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œå‡º ---
    new_filenames = [f.name for f in uploaded_spectrum_files] if uploaded_spectrum_files else []
    prev_filenames = st.session_state.get("uploaded_filenames", [])

    # --- è¨­å®šå¤‰æ›´æ¤œå‡º ---
    config_keys = ["spectrum_type_select", "second_deriv_smooth", "second_deriv_threshold", "prominence_threshold"]
    config_changed = any(
        st.session_state.get(f"prev_{key}") != st.session_state[key] for key in config_keys
    )
    file_changed = new_filenames != prev_filenames

    # --- æ‰‹å‹•ãƒ”ãƒ¼ã‚¯åˆæœŸåŒ–æ¡ä»¶ ---
    if config_changed or file_changed:
        for key in list(st.session_state.keys()):
            if key.endswith("_manual_peaks"):
                del st.session_state[key]
        st.session_state["manual_peak_keys"] = []
        st.session_state["uploaded_filenames"] = new_filenames
        for k in config_keys:
            st.session_state[f"prev_{k}"] = st.session_state[k]
            
    file_labels = []
    all_spectra = []
    all_bsremoval_spectra = []
    all_averemoval_spectra = []
    all_wavenum = []
    
    if uploaded_spectrum_files:
        config_keys = [
            "spectrum_type_select",
            "second_deriv_smooth",
            "second_deriv_threshold",
            "prominence_threshold"
        ]
        # ã‚»ãƒ¼ãƒ•ãªä»£å…¥å‡¦ç†ï¼ˆKeyErroré˜²æ­¢ï¼‰
        for k in config_keys:
            st.session_state[f"prev_{k}"] = st.session_state.get(k)

        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œå‡º ---
        file_changed = new_filenames != prev_filenames
        
        # --- æ‰‹å‹•ãƒ”ãƒ¼ã‚¯åˆæœŸåŒ–æ¡ä»¶ ---
        if config_changed or file_changed:
            for key in list(st.session_state.keys()):
                if key.endswith("_manual_peaks"):
                    del st.session_state[key]
            st.session_state["manual_peak_keys"] = []
            st.session_state["uploaded_filenames"] = new_filenames
        
            # å®‰å…¨ã« prev_ ã‚’ã‚»ãƒƒãƒˆ
            for k in config_keys:
                st.session_state[f"prev_{k}"] = st.session_state.get(k)
                
        for uploaded_file in uploaded_spectrum_files:
            file_name = uploaded_file.name
            file_extension = file_name.split('.')[-1] if '.' in file_name else ''

            try:
                data = read_csv_file(uploaded_file, file_extension)
                file_type = detect_file_type(data)
                uploaded_file.seek(0)
                if file_type == "unknown":
                    st.error(f"{file_name}ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥ã§ãã¾ã›ã‚“ã€‚")
                    continue

                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¯¾ã™ã‚‹å‡¦ç†
                if file_type == "wasatch":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Wasatch ENLIGHTEN - {file_name}")
                    lambda_ex = 785
                    data = pd.read_csv(uploaded_file, encoding='shift-jis', skiprows=46)
                    pre_wavelength = np.array(data["Wavelength"].values)
                    pre_wavenum = (1e7 / lambda_ex) - (1e7 / pre_wavelength)
                    pre_spectra = np.array(data["Processed"].values)

                elif file_type == "ramaneye_old":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data - {file_name}")
                    pre_wavenum = data["WaveNumber"]
                    pre_spectra = np.array(data.iloc[:, -1])
                    if pre_wavenum.iloc[0] > pre_wavenum.iloc[1]:
                        # pre_wavenum ã¨ pre_spectra ã‚’åè»¢
                        pre_wavenum = pre_wavenum[::-1]
                        pre_spectra = pre_spectra[::-1]
                        
                elif file_type == "ramaneye_new":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data - {file_name}")
                    
                    data = pd.read_csv(uploaded_file, skiprows=9)
                    pre_wavenum = data["WaveNumber"]
                    pre_spectra = np.array(data.iloc[:, -1])

                    if pre_wavenum.iloc[0] > pre_wavenum.iloc[1]:
                        # pre_wavenum ã¨ pre_spectra ã‚’åè»¢
                        pre_wavenum = pre_wavenum[::-1]
                        pre_spectra = pre_spectra[::-1]
                        
                elif file_type == "eagle":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Eagle Data - {file_name}")
                    data_transposed = data.transpose()
                    header = data_transposed.iloc[:3]  # æœ€åˆã®3è¡Œ
                    reversed_data = data_transposed.iloc[3:].iloc[::-1]
                    data_transposed = pd.concat([header, reversed_data], ignore_index=True)
                    pre_wavenum = np.array(data_transposed.iloc[3:, 0])
                    pre_spectra = np.array(data_transposed.iloc[3:, 1])
                
                start_index = find_index(pre_wavenum, start_wavenum)
                end_index = find_index(pre_wavenum, end_wavenum)

                wavenum = np.array(pre_wavenum[start_index:end_index+1])
                spectra = np.array(pre_spectra[start_index:end_index+1])

                # Baseline and spike removal 
                spectra_spikerm = remove_outliers_and_interpolate(spectra)
                mveAve_spectra = signal.medfilt(spectra_spikerm, savgol_wsize)
                lambda_ = 10e2
                baseline = airPLS(mveAve_spectra, dssn_th, lambda_, 2, 30)
                BSremoval_specta = spectra_spikerm - baseline
                BSremoval_specta_pos = BSremoval_specta + abs(np.minimum(spectra_spikerm, 0))  # è² å€¤ã‚’è£œæ­£

                # ç§»å‹•å¹³å‡å¾Œã®ã‚¹ãƒšã‚¯ãƒˆãƒ«
                Averemoval_specta = mveAve_spectra  - baseline
                Averemoval_specta_pos = Averemoval_specta + abs(np.minimum(mveAve_spectra, 0))  # è² å€¤ã‚’è£œæ­£

                # å„ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ ¼ç´
                file_labels.append(file_name)  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
                all_wavenum.append(wavenum)
                all_spectra.append(spectra)
                all_bsremoval_spectra.append(BSremoval_specta_pos)
                all_averemoval_spectra.append(Averemoval_specta_pos)
                
            except Exception as e:
                st.error(f"{file_name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®å®Ÿè¡Œ
        if 'peak_detection_triggered' not in st.session_state:
            st.session_state['peak_detection_triggered'] = False
    
        # ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã‚‰ãƒˆãƒªã‚¬ãƒ¼ã‚’ONã«
        if st.button("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œ"):
            st.session_state['peak_detection_triggered'] = True
        
        if st.session_state['peak_detection_triggered']:
            st.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ")
            
            peak_results = []
            
            # ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º
            st.info(f"""
            **æ¤œå‡ºè¨­å®š:**
            - ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—: {spectrum_type}
            - 2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–: {second_deriv_smooth}, é–¾å€¤: {second_deriv_threshold} (ãƒ”ãƒ¼ã‚¯æ¤œå‡ºç”¨)
            - ãƒ”ãƒ¼ã‚¯å“ç«‹åº¦é–¾å€¤: {peak_prominence_threshold}
            """)
            
            for i, file_name in enumerate(file_labels):
                # é¸æŠã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
                if spectrum_type == "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤":
                    selected_spectrum = all_bsremoval_spectra[i]
                else:  # ç§»å‹•å¹³å‡å¾Œ
                    selected_spectrum = all_averemoval_spectra[i]
                
                wavenum = all_wavenum[i]
                
                # 2æ¬¡å¾®åˆ†è¨ˆç®—ï¼ˆãƒ”ãƒ¼ã‚¯æ¤œå‡ºç”¨ï¼‰
                if len(selected_spectrum) > second_deriv_smooth:
                    second_derivative = savgol_filter(selected_spectrum, second_deriv_smooth, 2, deriv=2)
                else:
                    second_derivative = np.gradient(np.gradient(selected_spectrum))
                
                # 2æ¬¡å¾®åˆ†ã®ã¿ã«ã‚ˆã‚‹ãƒ”ãƒ¼ã‚¯æ¤œå‡ºï¼ˆprominenceåˆ¤å®šä»˜ã, propertiesã¯ãƒ€ãƒŸãƒ¼ï¼‰
                peaks, properties = find_peaks(-second_derivative, height=second_deriv_threshold)
                all_peaks, properties = find_peaks(-second_derivative)

                if len(peaks) > 0:
                    # Peak prominences ã‚’è¨ˆç®—
                    prominences = peak_prominences(-second_derivative, peaks)[0]
                    all_prominences = peak_prominences(-second_derivative, all_peaks)[0]

                    # Prominence é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    mask = prominences > peak_prominence_threshold
                    filtered_peaks = peaks[mask]
                    filtered_prominences = prominences[mask]
                    
                    # Â±2ãƒ”ã‚¯ã‚»ãƒ«ç¯„å›²å†…ã§å±€æ‰€æ¥µå¤§ã‚’å†ç¢ºèªãƒ»è£œæ­£
                    corrected_peaks = []
                    corrected_prominences = []
                    
                    for peak_idx, prom in zip(filtered_peaks, filtered_prominences):
                        # Â±2ã®ç¯„å›²å†…ã§æœ€å¤§å€¤ã‚’æ¢ã™ï¼ˆç¯„å›²è¶…ãˆãªã„ã‚ˆã†åˆ¶é™ï¼‰
                        window_start = max(0, peak_idx - 2)
                        window_end = min(len(selected_spectrum), peak_idx + 3)
                        local_window = selected_spectrum[window_start:window_end]
                        
                        # å±€æ‰€æœ€å¤§å€¤ã®ä½ç½®ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
                        local_max_idx = np.argmax(local_window)
                        corrected_idx = window_start + local_max_idx
                    
                        corrected_peaks.append(corrected_idx)
                        
                        # Prominence ã‚‚å†è¨ˆç®—
                        local_prom = peak_prominences(-second_derivative, [corrected_idx])[0][0]
                        corrected_prominences.append(local_prom)
                    
                    # numpyé…åˆ—ã«å¤‰æ›
                    filtered_peaks = np.array(corrected_peaks)
                    filtered_prominences = np.array(corrected_prominences)
                else:
                    filtered_peaks = np.array([])
                    filtered_prominences = np.array([])
                
                # çµæœã‚’ä¿å­˜
                peak_data = {
                    'file_name': file_name,
                    'detected_peaks': filtered_peaks,
                    'detected_prominences': filtered_prominences,
                    'wavenum': wavenum,
                    'spectrum': selected_spectrum,
                    'second_derivative': second_derivative,
                    'second_deriv_smooth': second_deriv_smooth,
                    'second_deriv_threshold': second_deriv_threshold,
                    'prominence_threshold': peak_prominence_threshold,
                    'all_peaks': all_peaks,
                    'all_prominences': all_prominences,
                }
                peak_results.append(peak_data)
                
                # çµæœã‚’è¡¨ç¤º
                st.write(f"**{file_name}**")
                st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æ•°: {len(filtered_peaks)} (2æ¬¡å¾®åˆ† + prominenceåˆ¤å®š)")
                
                # ãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
                if len(filtered_peaks) > 0:
                    peak_wavenums = wavenum[filtered_peaks]
                    peak_intensities = selected_spectrum[filtered_peaks]
                    st.write("**æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯:**")
                    peak_table = pd.DataFrame({
                        'ãƒ”ãƒ¼ã‚¯ç•ªå·': range(1, len(peak_wavenums) + 1),
                        'æ³¢æ•° (cmâ»Â¹)': [f"{wn:.1f}" for wn in peak_wavenums],
                        'å¼·åº¦': [f"{intensity:.3f}" for intensity in peak_intensities],
                        'Prominence': [f"{prom:.4f}" for prom in filtered_prominences]
                    })
                    st.table(peak_table)
                else:
                    st.write("ãƒ”ãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®æç”»
            for result in peak_results:
                file_key = result['file_name']
            
                # åˆæœŸåŒ–ï¼ˆé™¤å¤–å¯¾è±¡ãƒªã‚¹ãƒˆã¨æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ï¼‰
                if f"{file_key}_excluded_peaks" not in st.session_state:
                    st.session_state[f"{file_key}_excluded_peaks"] = set()
                if f"{file_key}_manual_peaks" not in st.session_state:
                    st.session_state[f"{file_key}_manual_peaks"] = []
            
                # é™¤å¤–ã‚’åæ˜ ã—ãŸãƒ”ãƒ¼ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                filtered_peaks = [
                    i for i in result['detected_peaks']
                    if i not in st.session_state[f"{file_key}_excluded_peaks"]
                ]
                filtered_prominences = [
                    prom for i, prom in zip(result['detected_peaks'], result['detected_prominences'])
                    if i not in st.session_state[f"{file_key}_excluded_peaks"]
                ]
            
                fig = make_subplots(
                    rows=3, cols=1,
                    shared_xaxes=True,
                    subplot_titles=[
                        f'{file_key} - {spectrum_type}',
                        f'{file_key} - å¾®åˆ†ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¯”è¼ƒ',
                        f'{file_key} - Prominence vs æ³¢æ•°'
                    ],
                    vertical_spacing=0.07,
                    row_heights=[0.4, 0.3, 0.3]
                )
            
                # --- ä¸Šæ®µã‚¹ãƒšã‚¯ãƒˆãƒ« ---
                fig.add_trace(
                    go.Scatter(
                        x=result['wavenum'],
                        y=result['spectrum'],
                        mode='lines',
                        name=spectrum_type,
                        line=dict(color='blue', width=1)
                    ),
                    row=1, col=1
                )
            
                # è‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯ï¼ˆæœ‰åŠ¹ãªã‚‚ã®ã®ã¿ï¼‰
                if len(filtered_peaks) > 0:
                    fig.add_trace(
                        go.Scatter(
                            x=result['wavenum'][filtered_peaks],
                            y=result['spectrum'][filtered_peaks],
                            mode='markers',
                            name='æ¤œå‡ºãƒ”ãƒ¼ã‚¯ï¼ˆæœ‰åŠ¹ï¼‰',
                            marker=dict(color='red', size=8, symbol='circle')
                        ),
                        row=1, col=1
                    )
            
                # é™¤å¤–ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ï¼ˆÃ—ãƒãƒ¼ã‚¯ã§è¡¨ç¤ºï¼‰
                excluded_peaks = list(st.session_state[f"{file_key}_excluded_peaks"])
                if len(excluded_peaks) > 0:
                    fig.add_trace(
                        go.Scatter(
                            x=result['wavenum'][excluded_peaks],
                            y=result['spectrum'][excluded_peaks],
                            mode='markers',
                            name='é™¤å¤–ãƒ”ãƒ¼ã‚¯',
                            marker=dict(color='gray', size=8, symbol='x')
                        ),
                        row=1, col=1
                    )
            
                # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯
                for x, y in st.session_state[f"{file_key}_manual_peaks"]:
                    fig.add_trace(
                        go.Scatter(
                            x=[x],
                            y=[y],
                            mode='markers+text',
                            marker=dict(color='green', size=10, symbol='star'),
                            text=["æ‰‹å‹•"],
                            textposition='top center',
                            name="æ‰‹å‹•ãƒ”ãƒ¼ã‚¯",
                            showlegend=False
                        ),
                        row=1, col=1
                    )
            
                # --- 2æ¬¡å¾®åˆ† ---
                fig.add_trace(
                    go.Scatter(
                        x=result['wavenum'],
                        y=result['second_derivative'],
                        mode='lines',
                        name='2æ¬¡å¾®åˆ†',
                        line=dict(color='purple', width=1)
                    ),
                    row=2, col=1
                )
            
                fig.add_hline(y=0, line_dash="dash", line_color="black", opacity=0.5, row=2, col=1)
            
                # --- Prominenceãƒ—ãƒ­ãƒƒãƒˆ ---
                fig.add_trace(
                    go.Scatter(
                        x=result['wavenum'][result['all_peaks']],
                        y=result['all_prominences'],
                        mode='markers',
                        name='å…¨ãƒ”ãƒ¼ã‚¯ã®Prominence',
                        marker=dict(color='orange', size=4)
                    ),
                    row=3, col=1
                )
                if len(filtered_peaks) > 0:
                    fig.add_trace(
                        go.Scatter(
                            x=result['wavenum'][filtered_peaks],
                            y=filtered_prominences,
                            mode='markers',
                            name='æœ‰åŠ¹ãªProminence',
                            marker=dict(color='red', size=7, symbol='circle')
                        ),
                        row=3, col=1
                    )
            
                fig.update_layout(height=800, margin=dict(t=80, b=40))
                fig.update_xaxes(title_text="æ³¢æ•° (cmâ»Â¹)", row=3, col=1)
                fig.update_yaxes(title_text="å¼·åº¦", row=1, col=1)
                fig.update_yaxes(title_text="å¾®åˆ†å€¤", row=2, col=1)
                
                # ç¾åœ¨ã® file_key ã‚’åˆ©ç”¨
                event_key = f"{file_key}_click_event"
                
                # ã‚¯ãƒªãƒƒã‚¯å–å¾—
                clicked_points = plotly_events(
                    fig,
                    click_event=True,
                    hover_event=False,
                    select_event=False,
                    override_height=800,
                    key=event_key
                )
                
                # event_key ã‚’å®šç¾©
                event_key = f"{file_key}_click_event"
                
                # curveNumber==0ï¼ˆå…ƒã‚¹ãƒšã‚¯ãƒˆãƒ«ï¼‰ã®ã‚¯ãƒªãƒƒã‚¯ã ã‘å‡¦ç†
                clicked_main = [pt for pt in clicked_points if pt["curveNumber"] == 0]
                
                if clicked_main:
                    pt = clicked_main[-1]
                    click_id = str(pt['x']) + str(pt['y'])  # click_id ã®ä»£æ›¿
                
                    last_click_id = st.session_state.get(f"{event_key}_last", None)
                    if click_id != last_click_id:
                        st.session_state[f"{event_key}_last"] = click_id
                
                        x = pt['x']
                        y = pt['y']
                        wavenum_arr = result['wavenum']
                        spectrum_arr = result['spectrum']
                        idx = np.argmin(np.abs(wavenum_arr - x))
                
                        # è‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯ãªã‚‰ãƒˆã‚°ãƒ«
                        if idx in result['detected_peaks']:
                            if idx in st.session_state[f"{file_key}_excluded_peaks"]:
                                st.session_state[f"{file_key}_excluded_peaks"].remove(idx)
                            else:
                                st.session_state[f"{file_key}_excluded_peaks"].add(idx)
                
                        else:
                            # ã™ã§ã«åŒã˜å ´æ‰€ã«æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Œã°è¿½åŠ ã—ãªã„
                            is_duplicate = any(abs(existing_x - x) < 1.0 for existing_x, _ in st.session_state[f"{file_key}_manual_peaks"])
                            if not is_duplicate:
                                st.session_state[f"{file_key}_manual_peaks"].append((x, y))
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ãƒ©ã‚°ã®åˆæœŸåŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ï¼‰
                if f"show_info_{file_key}" not in st.session_state:
                    st.session_state[f"show_info_{file_key}"] = False
                
                # ã€Œæ‰‹å‹•ãƒ”ãƒ¼ã‚¯ã®æƒ…å ±ã‚’è¡¨ç¤ºã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã‚‰ãƒ•ãƒ©ã‚°ã‚’Trueã«
                if st.button("ğŸ” æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ã®æƒ…å ±ã‚’è¡¨ç¤º", key=f"show_manual_info_{file_key}"):
                    st.session_state[f"show_info_{file_key}"] = True
                
                # è¡¨ç¤ºãƒ•ãƒ©ã‚°ã«å¿œã˜ã¦æ‰‹å‹•æƒ…å ±ã¨ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå‡¦ç†ã‚’å®Ÿè¡Œ
                if st.session_state[f"show_info_{file_key}"]:
                
                    manual_peaks = st.session_state.get(f"{file_key}_manual_peaks", [])
                    excluded_peaks = st.session_state.get(f"{file_key}_excluded_peaks", set())
                
                    wavenum = result['wavenum']
                    spectrum = result['spectrum']
                    second_derivative = result['second_derivative']
                
                    # --- æ‰‹å‹•ã§è¿½åŠ ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æƒ…å ± ---
                    manual_peak_table = []
                    if manual_peaks:
                        for x, y in manual_peaks:
                            idx = np.argmin(np.abs(wavenum - x))
                            window_size = 3
                            local_start = max(0, idx - window_size)
                            local_end = min(len(second_derivative), idx + window_size + 1)
                            local_window = -second_derivative[local_start:local_end]
                            local_max_idx = np.argmax(local_window)
                            peak_idx = local_start + local_max_idx
                
                            try:
                                prom = peak_prominences(-second_derivative, [peak_idx])[0][0]
                            except Exception:
                                prom = 0.0
                
                            manual_peak_table.append({
                                "æ³¢æ•° (cmâ»Â¹)": f"{x:.1f}",
                                "å¼·åº¦": f"{y:.3f}",
                                "Prominence": f"{prom:.3f}"
                            })
                
                        st.write(f"**{file_key} ã®æ‰‹å‹•ã§è¿½åŠ ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯:**")
                        st.table(pd.DataFrame(manual_peak_table))
                    else:
                        st.info("æ‰‹å‹•ã§è¿½åŠ ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                
                    # --- æ‰‹å‹•ã§é™¤å¤–ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æƒ…å ± ---
                    excluded_table = []
                    if excluded_peaks:
                        for idx in sorted(excluded_peaks):
                            if idx < len(wavenum):
                                x = wavenum[idx]
                                y = spectrum[idx]
                                try:
                                    prom = peak_prominences(-second_derivative, [idx])[0][0]
                                except Exception:
                                    prom = 0.0
                
                                excluded_table.append({
                                    "æ³¢æ•° (cmâ»Â¹)": f"{x:.1f}",
                                    "å¼·åº¦": f"{y:.3f}",
                                    "Prominence": f"{prom:.3f}"
                                })
                
                        st.write(f"**{file_key} ã®æ‰‹å‹•ã§é™¤å¤–ã•ã‚ŒãŸï¼ˆé™¤å¤–ãƒãƒ¼ã‚¯ä»˜ãï¼‰ãƒ”ãƒ¼ã‚¯:**")
                        st.table(pd.DataFrame(excluded_table))
                    else:
                        st.info("æ‰‹å‹•ã§é™¤å¤–ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                
                    # æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯ã‚’ (x, y) ã®ã‚¿ãƒ—ãƒ«ã«å¤‰æ›
                    manual_add = [
                        (float(row["æ³¢æ•° (cmâ»Â¹)"]), float(row["å¼·åº¦"]))
                        for row in manual_peak_table
                    ]
                
                    # é™¤å¤–ãƒ”ãƒ¼ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ³¢æ•°ã‹ã‚‰é€†å¼•ãã—ã¦ index ã‚’è¨ˆç®—ï¼‰
                    manual_exclude = set()
                    for row in excluded_table:
                        x = float(row["æ³¢æ•° (cmâ»Â¹)"])
                        idx4 = np.argmin(np.abs(wavenum - x))
                        manual_exclude.add(idx4)
                
                    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒå®Ÿè¡Œãƒœã‚¿ãƒ³
                    if st.button("ğŸ” ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒé©ç”¨ï¼ˆæœ€é©é–¾å€¤ã‚’æ¢ç´¢ï¼‰", key=f"optimize_{file_key}"):
                        result_opt = optimize_thresholds_via_gridsearch(
                            wavenum=wavenum,
                            spectrum=spectrum,
                            second_derivative=second_derivative,
                            manual_add_peaks=manual_add,
                            manual_exclude_indices=manual_exclude,
                            current_prom_thres=st.session_state['prominence_threshold'],
                            current_deriv_thres=st.session_state['second_deriv_threshold'],
                            detected_original_peaks=result["detected_peaks"],
                            resolution=40
                        )
                    
                        st.session_state[f"{file_key}_grid_result"] = result_opt

                        # `_temp` ã«ä¿å­˜ï¼ˆUIã«ã¯ã“ã®å€¤ãŒåæ˜ ã•ã‚Œã‚‹ï¼‰
                        st.session_state["prominence_threshold_temp"] = float(result_opt["prominence_threshold"])
                        st.session_state["second_deriv_threshold_temp"] = float(result_opt["second_deriv_threshold"])
                        
                        st.rerun()
                    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒçµæœãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ã‚Œã°è¡¨ç¤º
                    if f"{file_key}_grid_result" in st.session_state:
                        result_grid = st.session_state[f"{file_key}_grid_result"]
                        st.success(f"""
                        âœ… ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒæœ€é©åŒ–çµæœ:
                        - Prominence: {result_grid['prominence_threshold']:.4f}
                        - å¾®åˆ†é–¾å€¤: {result_grid['second_deriv_threshold']:.4f}
                        - ã‚¹ã‚³ã‚¢: {result_grid['score']}
                        """)
                    
                        # æœ€é©å€¤ã‚’å…¥åŠ›æ¬„ã«åæ˜ ï¼ˆãŸã ã— "åˆå›ã®ã¿" ã®åˆ¶é™ã‚ã‚Šï¼‰
                        if "prominence_threshold" not in st.session_state:
                            st.session_state["prominence_threshold"] = result_grid["prominence_threshold"]
                        if "second_deriv_threshold" not in st.session_state:
                            st.session_state["second_deriv_threshold"] = result_grid["second_deriv_threshold"]  
                    
                        # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å†å®Ÿè¡Œã™ã‚‹ãƒœã‚¿ãƒ³ï¼ˆæ‰‹å‹•è¿½åŠ ãƒ»é™¤å¤–ã‚‚ãƒªã‚»ãƒƒãƒˆï¼‰
                        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒçµæœã§å†æ¤œå‡º
                        if st.button("ğŸ”„ ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒçµæœã§å†æ¤œå‡º", key=f"reapply_{file_key}"):
                            st.session_state[f"{file_key}_manual_peaks"] = []
                            st.session_state[f"{file_key}_excluded_peaks"] = set()
                        
                            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¸€æ™‚çš„ã«ä¿å­˜ï¼ˆç›´æ¥å†ä»£å…¥ã—ãªã„ï¼‰
                            st.session_state["prominence_threshold_temp"] = float(result_grid["prominence_threshold"])
                            st.session_state["second_deriv_threshold_temp"] = float(result_grid["second_deriv_threshold"])
                        
                            # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºå†å®Ÿè¡Œãƒ•ãƒ©ã‚°
                            st.session_state["peak_detection_triggered"] = True
                        
                            st.rerun()
                
                # AIè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³ - ãƒ”ãƒ¼ã‚¯ç¢ºå®šå¾Œã®è€ƒå¯Ÿæ©Ÿèƒ½
                st.markdown("---")
                st.subheader(f"ğŸ¤– AIè§£æ - {file_key}")
                
                # æœ€çµ‚çš„ãªãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’åé›†ï¼ˆè‡ªå‹•æ¤œå‡º + æ‰‹å‹•è¿½åŠ  - é™¤å¤–ï¼‰
                final_peak_data = []
                
                # æœ‰åŠ¹ãªè‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯
                for idx, prom in zip(filtered_peaks, filtered_prominences):
                    final_peak_data.append({
                        'wavenumber': result['wavenum'][idx],
                        'intensity': result['spectrum'][idx],
                        'prominence': prom,
                        'type': 'auto'
                    })
                
                # æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯
                for x, y in st.session_state[f"{file_key}_manual_peaks"]:
                    idx = np.argmin(np.abs(result['wavenum'] - x))
                    try:
                        prom = peak_prominences(-result['second_derivative'], [idx])[0][0]
                    except:
                        prom = 0.0
                    
                    final_peak_data.append({
                        'wavenumber': x,
                        'intensity': y,
                        'prominence': prom,
                        'type': 'manual'
                    })
                
                if final_peak_data:
                    st.write(f"**æœ€çµ‚ç¢ºå®šãƒ”ãƒ¼ã‚¯æ•°: {len(final_peak_data)}**")
                    
                    # ãƒ”ãƒ¼ã‚¯è¡¨ç¤º
                    peak_summary_df = pd.DataFrame([
                        {
                            'ãƒ”ãƒ¼ã‚¯ç•ªå·': i+1,
                            'æ³¢æ•° (cmâ»Â¹)': f"{peak['wavenumber']:.1f}",
                            'å¼·åº¦': f"{peak['intensity']:.3f}",
                            'Prominence': f"{peak['prominence']:.3f}",
                            'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak['type'] == 'auto' else 'æ‰‹å‹•è¿½åŠ '
                        }
                        for i, peak in enumerate(final_peak_data)
                    ])
                    
                    # Mistral LLMã®åˆæœŸåŒ–ï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
                    if st.session_state.mistral_llm is None:
                        with st.spinner(f"Mistralãƒ¢ãƒ‡ãƒ« ({selected_model}) ã‚’åˆæœŸåŒ–ä¸­..."):
                            st.session_state.mistral_llm = HuggingFaceMistralLLM(selected_model)
                    
                    # AIè§£æå®Ÿè¡Œãƒœã‚¿ãƒ³
                    if st.button(f"ğŸ§  AIè§£æã‚’å®Ÿè¡Œ - {file_key}", key=f"ai_analysis_{file_key}", disabled=not final_peak_data):
                        with st.spinner("Hugging Face Mistralã§è§£æä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."):
                            analysis_report = None
                            start_time = time.time()
                    
                            try:
                                analyzer = RamanSpectrumAnalyzer()
                    
                                # é–¢é€£æ–‡çŒ®ã‚’æ¤œç´¢ï¼ˆä¸Šä½5ä»¶ï¼‰
                                search_terms = ' '.join([f"{p['wavenumber']:.0f}cm-1" for p in final_peak_data[:5]])
                                search_query = f"ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ ãƒ”ãƒ¼ã‚¯ {search_terms}"
                                relevant_docs = st.session_state.rag_system.search_relevant_documents(search_query, top_k=5)
                    
                                # AIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
                                analysis_prompt = analyzer.generate_analysis_prompt(
                                    peak_data=final_peak_data,
                                    relevant_docs=relevant_docs,
                                    user_hint=user_hint
                                )
                                
                                # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ç”¨ã‚¨ãƒªã‚¢
                                st.success("âœ… Mistralã®å¿œç­”ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºï¼‰")
                                stream_area = st.empty()
                                full_response = ""
                    
                                # Mistralã«ã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã§å•ã„åˆã‚ã›
                                system_prompt = "ã‚ãªãŸã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ”ãƒ¼ã‚¯ä½ç½®ã¨è«–æ–‡ã‚’æ¯”è¼ƒã—ã¦ã€ã“ã®ã‚µãƒ³ãƒ—ãƒ«ãŒä½•ã®è©¦æ–™ãªã®ã‹å½“ã¦ã¦ãã ã•ã„ã€‚ã™ã¹ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
                                full_prompt = f"{system_prompt}\n\n{analysis_prompt}\n\nã™ã¹ã¦æ—¥æœ¬èªã§è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
                                
                                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã®ç”Ÿæˆã¨è¡¨ç¤º
                                for chunk in st.session_state.mistral_llm.generate_stream_response(full_prompt, max_tokens=1024):
                                    full_response += chunk
                                    stream_area.markdown(full_response)
                    
                                # ãƒ”ãƒ¼ã‚¯æƒ…å ±ã¾ã¨ã‚è¡¨
                                peak_summary_df = pd.DataFrame([
                                    {
                                        'ãƒ”ãƒ¼ã‚¯ç•ªå·': i + 1,
                                        'æ³¢æ•° (cmâ»Â¹)': f"{peak['wavenumber']:.1f}",
                                        'å¼·åº¦': f"{peak['intensity']:.3f}",
                                        'Prominence': f"{peak['prominence']:.3f}",
                                        'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak['type'] == 'auto' else 'æ‰‹å‹•è¿½åŠ '
                                    }
                                    for i, peak in enumerate(final_peak_data)
                                ])
                    
                                # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                                analysis_report = f"""ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¬ãƒãƒ¼ãƒˆ
ãƒ•ã‚¡ã‚¤ãƒ«å: {file_key}
è§£ææ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model}

=== æ¤œå‡ºãƒ”ãƒ¼ã‚¯æƒ…å ± ===
{peak_summary_df.to_string(index=False)}

=== AIè§£æçµæœ ===
{full_response}

=== å‚ç…§æ–‡çŒ® ===
"""
                                for i, doc in enumerate(relevant_docs, 1):
                                    analysis_report += f"{i}. {doc['metadata']['filename']}ï¼ˆé¡ä¼¼åº¦: {doc['similarity_score']:.3f}ï¼‰\n"
                    
                                # å‡¦ç†æ™‚é–“ã®è¡¨ç¤º
                                elapsed = time.time() - start_time
                                st.info(f"ğŸ•’ è§£æã«ã‹ã‹ã£ãŸæ™‚é–“: {elapsed:.2f} ç§’")
                                
                                # è§£æçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                                st.session_state[f"{file_key}_ai_analysis"] = {
                                    'analysis': full_response,
                                    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                    'model': selected_model,
                                    'peak_data': final_peak_data
                                }
                    
                                # ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                                st.download_button(
                                    label="ğŸ“„ è§£æãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                    data=analysis_report,
                                    file_name=f"raman_analysis_report_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                                    mime="text/plain",
                                    key=f"download_report_{file_key}"
                                )
                    
                            except Exception as e:
                                st.error("AIè§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                                st.code(str(e))
                                st.error("GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™ã‹ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„ã€‚")
                    
                    # éå»ã®è§£æçµæœè¡¨ç¤º
                    if f"{file_key}_ai_analysis" in st.session_state:
                        with st.expander("ğŸ“œ éå»ã®è§£æçµæœã‚’è¡¨ç¤º"):
                            past_analysis = st.session_state[f"{file_key}_ai_analysis"]
                            st.write(f"**è§£ææ—¥æ™‚:** {past_analysis['timestamp']}")
                            st.write(f"**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {past_analysis['model']}")
                            st.markdown("**è§£æçµæœ:**")
                            st.markdown(past_analysis['analysis'])
                
                else:
                    st.info("ç¢ºå®šã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€æ‰‹å‹•ã§ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
                
            # å…¨ãƒ”ãƒ¼ã‚¯çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ã™ã‚‹
            all_peaks_data = []
            for result in peak_results:
                # æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ï¼ˆ2æ¬¡å¾®åˆ† + prominenceåˆ¤å®šï¼‰
                if len(result['detected_peaks']) > 0:
                    peak_wavenums = result['wavenum'][result['detected_peaks']]
                    peak_intensities = result['spectrum'][result['detected_peaks']]
                    for j, (wn, intensity, prominence) in enumerate(zip(peak_wavenums, peak_intensities, result['detected_prominences'])):
                        all_peaks_data.append({
                            'ãƒ•ã‚¡ã‚¤ãƒ«å': result['file_name'],
                            'ãƒ”ãƒ¼ã‚¯ç•ªå·': j + 1,
                            'æ³¢æ•°_cm-1': f"{wn:.1f}",
                            'å¼·åº¦': f"{intensity:.6f}",
                            'Prominence': f"{prominence:.6f}",
                            'ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—': spectrum_type,
                            'æ¤œå‡ºæ–¹æ³•': '2æ¬¡å¾®åˆ†+prominence',
                            'å¹³æ»‘åŒ–æ•°å€¤': result['second_deriv_smooth'],
                            '2æ¬¡å¾®åˆ†é–¾å€¤': result['second_deriv_threshold'],
                            'Prominenceé–¾å€¤': result['prominence_threshold']
                        })
            
            if all_peaks_data:
                peaks_df = pd.DataFrame(all_peaks_data)
                csv = peaks_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"peak_detection_results_{spectrum_type}_prominence.csv",
                    mime="text/csv"
                )

def main():
    st.set_page_config(
        page_title="Enhanced Raman Peak Detection Tool with Hugging Face Mistral", 
        page_icon="ğŸ“Š", 
        layout="wide"
    )
    
    st.title("ğŸ“Š Enhanced Raman Peak Detection Tool with AI Analysis")
    st.markdown("### ğŸ¤– Powered by Hugging Face Mistral")
    st.markdown("---")
        
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ“‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    
    spectrum_analysis_mode()
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.sidebar.markdown("---")
    st.sidebar.markdown("""
    ### ğŸ“š ä½¿ç”¨æ–¹æ³•
    1. **Mistralãƒ¢ãƒ‡ãƒ«é¸æŠ**: ä½¿ç”¨ã™ã‚‹Hugging Face Mistralãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    2. **è«–æ–‡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: RAGæ©Ÿèƒ½ç”¨ã®è«–æ–‡PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    3. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰**: è«–æ–‡ã‹ã‚‰æ¤œç´¢ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
    4. **ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: è§£æã™ã‚‹ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    5. **ãƒ”ãƒ¼ã‚¯æ¤œå‡º**: è‡ªå‹•æ¤œå‡º + æ‰‹å‹•èª¿æ•´ã§ãƒ”ãƒ¼ã‚¯ã‚’ç¢ºå®š
    6. **AIè§£æå®Ÿè¡Œ**: ç¢ºå®šãƒ”ãƒ¼ã‚¯ã‚’åŸºã«MistralãŒè€ƒå¯Ÿã‚’ç”Ÿæˆ
    
    ### ğŸ”§ ã‚µãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
    - **ã‚¹ãƒšã‚¯ãƒˆãƒ«**: CSV, TXT (RamanEye, Wasatch, Eagleå¯¾å¿œ)
    - **è«–æ–‡**: PDF, DOCX, TXT
    
    ### âš ï¸ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
    - **GPUæ¨å¥¨**: Mistralãƒ¢ãƒ‡ãƒ«ã¯é«˜é€ŸåŒ–ã®ãŸã‚GPUä½¿ç”¨ã‚’æ¨å¥¨
    - **ãƒ¡ãƒ¢ãƒª**: 8GBä»¥ä¸Šã®RAM/VRAMã‚’æ¨å¥¨
    - **ä¾å­˜é–¢ä¿‚**: requirements.txtã®å…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦
    """)

if __name__ == "__main__":
    main()
