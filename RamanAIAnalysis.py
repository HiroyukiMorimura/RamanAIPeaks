# -*- coding: utf-8 -*-
"""
Created on Fri Jun 20 09:02:35 2025

@author: hiroy

Enhanced Raman Spectrum Analysis Tool with HuggingFace Mistral and RAG functionality
"""

import time
import numpy as np
import pandas as pd
import seaborn as sns
import streamlit as st
import scipy.signal as signal
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.signal import savgol_filter, find_peaks, peak_prominences
from scipy.sparse.linalg import spsolve
from scipy.sparse import csc_matrix, eye, diags

from streamlit_plotly_events import plotly_events

# Additional imports for LLM and RAG functionality
import os
import glob
import PyPDF2
import docx
from datetime import datetime
from typing import List, Dict, Optional
import faiss
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline
import numpy as np

# Set matplotlib style
plt.style.use('default')
sns.set_palette("husl")

# RAGæ©Ÿèƒ½ã®ã‚¯ãƒ©ã‚¹å®šç¾©ï¼ˆtransformersãƒ™ãƒ¼ã‚¹ï¼‰
class SimpleRAGSystem:
    def __init__(self, embedding_model_name='cl-tohoku/bert-base-japanese'):
        """
        RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ï¼ˆtransformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰

        Args:
            embedding_model_name: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
        """
        self.embedding_model_name = embedding_model_name
        self.tokenizer = None
        self.model = None
        self.vector_db = None
        self.documents = []
        self.document_metadata = []
        self.embedding_dim = None
        self._model_loaded = False

    def extract_text_from_file(self, file_path: str) -> str:
        try:
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext == '.pdf':
                return self._extract_from_pdf(file_path)
            elif file_ext == '.docx':
                return self._extract_from_docx(file_path)
            elif file_ext == '.txt':
                return self._extract_from_txt(file_path)
            else:
                return ""
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _extract_from_pdf(self, file_path: str) -> str:
        text = ""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
        except Exception as e:
            st.error(f"PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return text

    def _extract_from_docx(self, file_path: str) -> str:
        try:
            doc = docx.Document(file_path)
            return "\n".join([p.text for p in doc.paragraphs])
        except Exception as e:
            st.error(f"DOCXèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _extract_from_txt(self, file_path: str) -> str:
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='shift_jis') as file:
                    return file.read()
            except Exception as e:
                st.error(f"TXTèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return ""
        except Exception as e:
            st.error(f"TXTèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        words = text.split()
        chunks = []
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip():
                chunks.append(chunk)
        return chunks

    def _load_embedding_model(self):
        if not self._model_loaded:
            try:
                with st.spinner("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    cache_dir = os.path.join(os.getcwd(), "model_cache")
                    os.makedirs(cache_dir, exist_ok=True)

                    import fugashi  # Mecabç”¨ä¾å­˜ã‚’æ˜ç¤ºçš„ã«ãƒ­ãƒ¼ãƒ‰

                    self.tokenizer = AutoTokenizer.from_pretrained(
                        self.embedding_model_name,
                        cache_dir=cache_dir,
                        use_fast=False
                    )
                    self.model = AutoModel.from_pretrained(
                        self.embedding_model_name,
                        cache_dir=cache_dir,
                        torch_dtype=torch.float32,
                        device_map="cpu"
                    )
                    self.model.eval()
                    self._model_loaded = True
                    st.success("âœ… åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†")
            except Exception as e:
                st.error(f"åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                st.info("ğŸ’¡ RAGæ©Ÿèƒ½ã‚’ç„¡åŠ¹ã«ã—ã¦ãã ã•ã„")
                return False
        return True

    def _encode_texts(self, texts: List[str]) -> np.ndarray:
        if not self._model_loaded:
            return np.array([])
        embeddings = []
        batch_size = 8
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                )
                outputs = self.model(**inputs)
                attention_mask = inputs['attention_mask']
                token_embeddings = outputs.last_hidden_state
                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                batch_embeddings = sum_embeddings / sum_mask
                embeddings.append(batch_embeddings.cpu().numpy())
        return np.vstack(embeddings) if embeddings else np.array([])

    def build_vector_database(self, folder_path: str):
        if not self._load_embedding_model():
            return
        if not os.path.exists(folder_path):
            st.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return
        file_patterns = ['*.pdf', '*.docx', '*.txt']
        files = []
        for pattern in file_patterns:
            files.extend(glob.glob(os.path.join(folder_path, pattern)))
        if not files:
            st.warning("æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«è«–æ–‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        st.info(f"è«–æ–‡ãƒ•ã‚¡ã‚¤ãƒ« {len(files)} ä»¶ã‚’å‡¦ç†ä¸­...")
        all_chunks = []
        all_metadata = []
        progress_bar = st.progress(0)
        for i, file_path in enumerate(files):
            text = self.extract_text_from_file(file_path)
            if text:
                chunks = self.chunk_text(text)
                for chunk in chunks:
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'filename': os.path.basename(file_path),
                        'filepath': file_path,
                        'chunk_text': chunk[:100] + "..." if len(chunk) > 100 else chunk
                    })
            progress_bar.progress((i + 1) / len(files))
        if not all_chunks:
            st.error("å‡¦ç†å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return
        st.info("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­...")
        progress_bar2 = st.progress(0)
        embeddings = self._encode_texts(all_chunks)
        progress_bar2.progress(1.0)
        if len(embeddings) == 0:
            st.error("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return
        self.embedding_dim = embeddings.shape[1]
        self.vector_db = faiss.IndexFlatIP(self.embedding_dim)
        faiss.normalize_L2(embeddings.astype(np.float32))
        self.vector_db.add(embeddings.astype(np.float32))
        self.documents = all_chunks
        self.document_metadata = all_metadata
        st.success(f"ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹ç¯‰å®Œäº†: {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")

    def search_relevant_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        if self.vector_db is None:
            return []
        if not self._load_embedding_model():
            return []
        try:
            query_embeddings = self._encode_texts([query])
            if len(query_embeddings) == 0:
                return []
            faiss.normalize_L2(query_embeddings.astype(np.float32))
            scores, indices = self.vector_db.search(query_embeddings.astype(np.float32), top_k)
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents):
                    results.append({
                        'text': self.documents[idx],
                        'metadata': self.document_metadata[idx],
                        'similarity_score': float(score)
                    })
            return results
        except Exception as e:
            st.error(f"æ–‡æ›¸æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return []


class SimpleLLM:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªLLMã‚¯ãƒ©ã‚¹ï¼ˆè»½é‡ç‰ˆï¼‰
    """
    def __init__(self, model_name="cyberagent/open-calm-small"):
        self.model_name = model_name
        self.pipeline = None
        self._model_loaded = False

    def _load_model(self):
        if self._model_loaded:
            return True

        try:
            with st.spinner(f"è¨€èªãƒ¢ãƒ‡ãƒ« ({self.model_name}) ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                cache_dir = os.path.join(os.getcwd(), "model_cache")
                os.makedirs(cache_dir, exist_ok=True)

                tokenizer = AutoTokenizer.from_pretrained(self.model_name, cache_dir=cache_dir)
                model = AutoModelForCausalLM.from_pretrained(self.model_name, cache_dir=cache_dir)

                
                generator = pipeline(
                    "text-generation",
                    model="cyberagent/open-calm-small",
                    tokenizer="cyberagent/open-calm-small",
                    device=-1  # CPU
                )

                self._model_loaded = True
                st.success(f"âœ… è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return True

        except Exception as e:
            st.error(f"è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.info("ğŸ’¡ è§£æ±ºç­–: \n1. ãƒ¢ãƒ‡ãƒ«åã®ã‚¹ãƒšãƒ«ç¢ºèª\n2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç¢ºèª\n3. requirementsã‹ã‚‰sentencepieceã‚’å¤–ã™")
            return False
    def generate_response(self, prompt: str, max_tokens: int = 256) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self._load_model():
            return "âš ï¸ ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªãƒ”ãƒ¼ã‚¯è§£æçµæœã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚"
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ—¥æœ¬èªè§£æç”¨ã«æœ€é©åŒ–
            formatted_prompt = f"""ä»¥ä¸‹ã®ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€è©¦æ–™ã®æˆåˆ†ã‚’æ¨å®šã—ã¦ãã ã•ã„ã€‚

{prompt}

å›ç­”ã¯æ—¥æœ¬èªã§ã€ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ï¼š
1. å„ãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„å¸°å±
2. æ¨å®šã•ã‚Œã‚‹è©¦æ–™ã®ç¨®é¡
3. æ ¹æ‹ ã¨ãªã‚‹ãƒ”ãƒ¼ã‚¯ä½ç½®ã®è§£é‡ˆ

å›ç­”:"""
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            if "DialoGPT" in self.model_name:
                response = self.pipeline(
                    formatted_prompt,
                    max_length=len(formatted_prompt) + max_tokens,
                    num_return_sequences=1,
                    pad_token_id=self.pipeline.tokenizer.eos_token_id
                )
                generated_text = response[0]['generated_text']
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                result = generated_text[len(formatted_prompt):].strip()
            else:
                response = self.pipeline(
                    formatted_prompt,
                    max_new_tokens=max_tokens,
                    num_return_sequences=1,
                    pad_token_id=self.pipeline.tokenizer.eos_token_id
                )
                result = response[0]['generated_text'].strip()
            
            return result if result else "è§£æã‚’å®Ÿè¡Œã—ã¾ã—ãŸãŒã€å…·ä½“çš„ãªæ¨å®šçµæœã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        except Exception as e:
            return f"âš ï¸ å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
    
    def generate_stream_response(self, prompt: str, max_tokens: int = 256):
        full_response = self.generate_response(prompt, max_tokens)
    
        if not full_response:
            yield "âš ï¸ å¿œç­”ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            return
    
        for i in range(0, len(full_response), 5):
            chunk = full_response[i:i+5]
            yield chunk
            time.sleep(0.03)

class RamanSpectrumAnalyzer:
    def generate_analysis_prompt(
        self,
        peak_data: List[Dict],
        relevant_docs: List[Dict],
        user_hint: Optional[str] = None
    ) -> str:
        """
        ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ

        Args:
            peak_data: ãƒ”ãƒ¼ã‚¯æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            relevant_docs: é–¢é€£æ–‡çŒ®æƒ…å ±ï¼ˆRAGçµæœï¼‰
            user_hint: ãƒ¦ãƒ¼ã‚¶ãƒ¼è£œè¶³ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰

        Returns:
            LLMç”¨è§£æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
        """

        def format_peaks(peaks: List[Dict]) -> str:
            header = "ã€æ¤œå‡ºãƒ”ãƒ¼ã‚¯ä¸€è¦§ã€‘"
            lines = [
                f"{i+1}. æ³¢æ•°: {p.get('wavenumber', 0):.1f} cmâ»Â¹, "
                f"å¼·åº¦: {p.get('intensity', 0):.3f}, "
                f"prominence: {p.get('prominence', 0):.3f}, "
                f"ç¨®é¡: {'è‡ªå‹•æ¤œå‡º' if p.get('type') == 'auto' else 'æ‰‹å‹•è¿½åŠ '}"
                for i, p in enumerate(peaks)
            ]
            return "\n".join([header] + lines)

        def format_reference_excerpts(docs: List[Dict]) -> str:
            header = "ã€å¼•ç”¨æ–‡çŒ®ã®æŠœç²‹ã¨è¦ç´„ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                title = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                page = doc.get("metadata", {}).get("page")
                summary = doc.get("page_content", "").strip()
                lines.append(f"\n--- å¼•ç”¨{i} ---")
                lines.append(f"å‡ºå…¸ãƒ•ã‚¡ã‚¤ãƒ«: {title}")
                if page is not None:
                    lines.append(f"ãƒšãƒ¼ã‚¸ç•ªå·: {page}")
                lines.append(f"æŠœç²‹å†…å®¹:\n{summary}")
            return "\n".join([header] + lines)

        def format_doc_summaries(docs: List[Dict], preview_length: int = 300) -> str:
            header = "ã€æ–‡çŒ®ã®æ¦‚è¦ï¼ˆé¡ä¼¼åº¦ä»˜ãï¼‰ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                filename = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                similarity = doc.get("similarity_score", 0.0)
                text = doc.get("text") or doc.get("page_content") or ""
                lines.append(
                    f"æ–‡çŒ®{i} (é¡ä¼¼åº¦: {similarity:.3f})\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\n"
                    f"å†’é ­æŠœç²‹: {text.strip()[:preview_length]}...\n"
                )
            return "\n".join([header] + lines)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã®æ§‹ç¯‰
        sections = [
            "ä»¥ä¸‹ã¯ã€ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã§æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æƒ…å ±ã§ã™ã€‚",
            "ã“ã‚Œã‚‰ã®ãƒ”ãƒ¼ã‚¯ã«åŸºã¥ãã€è©¦æ–™ã®æˆåˆ†ã‚„ç‰¹å¾´ã«ã¤ã„ã¦æ¨å®šã—ã¦ãã ã•ã„ã€‚",
            "ãªãŠã€æ–‡çŒ®ã¨ã®æ¯”è¼ƒã«ãŠã„ã¦ã¯ãƒ”ãƒ¼ã‚¯ä½ç½®ãŒÂ±5cmâ»Â¹ç¨‹åº¦ãšã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãã®ãŸã‚ã€Â±5cmâ»Â¹ä»¥å†…ã®å·®ã§ã‚ã‚Œã°ä¸€è‡´ã¨ã¿ãªã—ã¦è§£æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n"
        ]

        if user_hint:
            sections.append(f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹è£œè¶³æƒ…å ±ã€‘\n{user_hint}\n")

        if peak_data:
            sections.append(format_peaks(peak_data))
        if relevant_docs:
            sections.append(format_reference_excerpts(relevant_docs))
            sections.append(format_doc_summaries(relevant_docs))

        sections.append(
            "ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€è©¦æ–™ã«å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹åŒ–åˆç‰©ã‚„ç‰©è³ªæ§‹é€ ã€ç‰¹å¾´ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n"
            "å‡ºåŠ›ã¯æ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n"
            "## è§£æã®è¦³ç‚¹:\n"
            "1. å„ãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„å¸°å±ã¨ãã®æ ¹æ‹ \n"
            "2. è©¦æ–™ã®å¯èƒ½ãªçµ„æˆã‚„æ§‹é€ \n"
            "3. æ–‡çŒ®æƒ…å ±ã¨ã®æ¯”è¼ƒãƒ»å¯¾ç…§\n\n"
            "è©³ç´°ã§ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸè€ƒå¯Ÿã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )

        return "\n".join(sections)
    
    def _generate_basic_analysis(self, peak_data: List[Dict]) -> str:
        """
        åŸºæœ¬çš„ãªãƒ”ãƒ¼ã‚¯è§£æã‚’ç”Ÿæˆï¼ˆAIä¸ä½¿ç”¨ï¼‰
        
        Args:
            peak_data: ãƒ”ãƒ¼ã‚¯æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            åŸºæœ¬è§£æãƒ†ã‚­ã‚¹ãƒˆ
        """
        # ä¸€èˆ¬çš„ãªãƒ©ãƒãƒ³ãƒ”ãƒ¼ã‚¯ã®è§£é‡ˆãƒ†ãƒ¼ãƒ–ãƒ«
        common_peaks = {
            (400, 500): "é‡‘å±é…¸åŒ–ç‰©ã®çµåˆæŒ¯å‹•",
            (500, 600): "S-Sçµåˆã€é‡‘å±-é…¸ç´ çµåˆ",
            (600, 800): "C-Sçµåˆã€èŠ³é¦™ç’°ã®å¤‰è§’æŒ¯å‹•",
            (800, 1000): "C-Cçµåˆã®ä¼¸ç¸®æŒ¯å‹•",
            (1000, 1200): "C-Oçµåˆã€C-Nçµåˆã®ä¼¸ç¸®æŒ¯å‹•",
            (1200, 1400): "C-Hçµåˆã®å¤‰è§’æŒ¯å‹•",
            (1400, 1600): "C=Cçµåˆã®ä¼¸ç¸®æŒ¯å‹•ï¼ˆèŠ³é¦™ç’°ï¼‰",
            (1600, 1800): "C=Oçµåˆã®ä¼¸ç¸®æŒ¯å‹•",
            (2800, 3000): "C-Hçµåˆã®ä¼¸ç¸®æŒ¯å‹•ï¼ˆã‚¢ãƒ«ã‚­ãƒ«ï¼‰",
            (3000, 3200): "C-Hçµåˆã®ä¼¸ç¸®æŒ¯å‹•ï¼ˆèŠ³é¦™ç’°ï¼‰",
            (3200, 3600): "O-Hçµåˆã®ä¼¸ç¸®æŒ¯å‹•",
        }
        
        analysis_parts = ["## æ¤œå‡ºãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„è§£é‡ˆ\n"]
        
        for i, peak in enumerate(peak_data, 1):
            wavenumber = peak['wavenumber']
            intensity = peak['intensity']
            peak_type = 'è‡ªå‹•æ¤œå‡º' if peak['type'] == 'auto' else 'æ‰‹å‹•è¿½åŠ '
            
            # ãƒ”ãƒ¼ã‚¯è§£é‡ˆã‚’æ¤œç´¢
            interpretations = []
            for (min_wn, max_wn), description in common_peaks.items():
                if min_wn <= wavenumber <= max_wn:
                    interpretations.append(description)
            
            analysis_parts.append(f"**ãƒ”ãƒ¼ã‚¯ {i} ({wavenumber:.1f} cmâ»Â¹, {peak_type})**")
            if interpretations:
                analysis_parts.append(f"- æ¨å®šçµåˆ: {', '.join(interpretations)}")
            else:
                analysis_parts.append("- æ¨å®šçµåˆ: ç‰¹å®šã®çµåˆã®åŒå®šãŒå›°é›£")
            
            # å¼·åº¦ã«ã‚ˆã‚‹è§£é‡ˆ
            if intensity > 0.8:
                analysis_parts.append("- å¼·åº¦: éå¸¸ã«å¼·ã„ï¼ˆä¸»è¦æˆåˆ†ã®ç‰¹å¾´çš„ãƒ”ãƒ¼ã‚¯ï¼‰")
            elif intensity > 0.5:
                analysis_parts.append("- å¼·åº¦: å¼·ã„ï¼ˆé‡è¦ãªæ§‹é€ æˆåˆ†ï¼‰")
            elif intensity > 0.2:
                analysis_parts.append("- å¼·åº¦: ä¸­ç¨‹åº¦ï¼ˆå‰¯æ¬¡çš„æˆåˆ†ï¼‰")
            else:
                analysis_parts.append("- å¼·åº¦: å¼±ã„ï¼ˆå¾®é‡æˆåˆ†ã¾ãŸã¯é›‘éŸ³ï¼‰")
            
            analysis_parts.append("")
        
        # å…¨ä½“çš„ãªå‚¾å‘åˆ†æ
        analysis_parts.append("## è©¦æ–™ã®ç‰¹å¾´æ¨å®š\n")
        
        wavenumbers = [p['wavenumber'] for p in peak_data]
        
        # æœ‰æ©Ÿç‰©vsç„¡æ©Ÿç‰©ã®åˆ¤å®š
        organic_count = sum(1 for wn in wavenumbers if 800 <= wn <= 3600)
        inorganic_count = sum(1 for wn in wavenumbers if 200 <= wn <= 800)
        
        if organic_count > inorganic_count:
            analysis_parts.append("- **æœ‰æ©ŸåŒ–åˆç‰©ã®ç‰¹å¾´ãŒå¼·ã„**: C-H, C=C, C=Oç­‰ã®æœ‰æ©ŸçµåˆãŒå¤šæ•°æ¤œå‡º")
        elif inorganic_count > organic_count:
            analysis_parts.append("- **ç„¡æ©ŸåŒ–åˆç‰©ã®ç‰¹å¾´ãŒå¼·ã„**: é‡‘å±é…¸åŒ–ç‰©ã‚„ç„¡æ©ŸçµåˆãŒä¸»è¦")
        else:
            analysis_parts.append("- **æœ‰æ©Ÿãƒ»ç„¡æ©Ÿæ··åˆç‰©ã®å¯èƒ½æ€§**: ä¸¡æ–¹ã®ç‰¹å¾´ã‚’å«æœ‰")
        
        # ç‰¹å®šã®åŒ–åˆç‰©ç¾¤ã®æ¨å®š
        if any(1300 <= wn <= 1400 for wn in wavenumbers):
            analysis_parts.append("- **ç‚­ç´ ç³»ææ–™ã®å¯èƒ½æ€§**: ã‚°ãƒ©ãƒ•ã‚¡ã‚¤ãƒˆç³»ææ–™ã®D-bandãŒæ¤œå‡ºã•ã‚Œã‚‹å¯èƒ½æ€§")
        
        if any(1580 <= wn <= 1590 for wn in wavenumbers):
            analysis_parts.append("- **ã‚°ãƒ©ãƒ•ã‚¡ã‚¤ãƒˆç³»ææ–™**: G-bandãŒæ¤œå‡ºã•ã‚Œã‚‹å¯èƒ½æ€§")
        
        if any(2800 <= wn <= 3000 for wn in wavenumbers):
            analysis_parts.append("- **é«˜åˆ†å­ææ–™ã®å¯èƒ½æ€§**: ã‚¢ãƒ«ã‚­ãƒ«åŸºC-Hä¼¸ç¸®ãŒæ¤œå‡º")
        
        analysis_parts.append("\n**æ³¨æ„**: ã“ã®è§£æã¯ä¸€èˆ¬çš„ãªãƒ”ãƒ¼ã‚¯å¸°å±ã«åŸºã¥ãæ¨å®šã§ã™ã€‚æ­£ç¢ºãªåŒå®šã«ã¯è¿½åŠ ã®åˆ†ææ‰‹æ³•ã¨ã®çµ„ã¿åˆã‚ã›ãŒå¿…è¦ã§ã™ã€‚")
        
        return "\n".join(analysis_parts)

# æ—¢å­˜ã®é–¢æ•°ç¾¤ï¼ˆå¤‰æ›´ãªã—ï¼‰
def create_features_labels(spectra, window_size=10):
    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®é…åˆ—ã‚’åˆæœŸåŒ–
    X = []
    y = []
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®é•·ã•
    n_points = len(spectra)
    # äººæ‰‹ã«ã‚ˆã‚‹ãƒ”ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«ã€ã¾ãŸã¯è‡ªå‹•ç”Ÿæˆã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«é…ç½®
    peak_labels = np.zeros(n_points)

    # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®æŠ½å‡º
    for i in range(window_size, n_points - window_size):
        # å‰å¾Œã®çª“ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨
        features = spectra[i-window_size:i+window_size+1]
        X.append(features)
        y.append(peak_labels[i])

    return np.array(X), np.array(y)

def find_index(rs_array,  rs_focused):
    '''
    Convert the index of the proximate wavenumber by finding the absolute 
    minimum value of (rs_array - rs_focused)
    
    input
        rs_array: Raman wavenumber
        rs_focused: Index
    output
        index
    '''

    diff = [abs(element - rs_focused) for element in rs_array]
    index = np.argmin(diff)
    return index

def WhittakerSmooth(x,w,lambda_,differences=1):
    '''
    Penalized least squares algorithm for background fitting
    
    input
        x: input data (i.e. chromatogram of spectrum)
        w: binary masks (value of the mask is zero if a point belongs to peaks and one otherwise)
        lambda_: parameter that can be adjusted by user. The larger lambda is,  the smoother the resulting background
        differences: integer indicating the order of the difference of penalties
    
    output
        the fitted background vector
    '''
    X=np.array(x, dtype=np.float64)
    m=X.size
    E=eye(m,format='csc')
    for i in range(differences):
        E=E[1:]-E[:-1] # numpy.diff() does not work with sparse matrix. This is a workaround.
    W=diags(w,0,shape=(m,m))
    A=csc_matrix(W+(lambda_*E.T*E))
    B=csc_matrix(W*X.T).toarray().flatten()
    background=spsolve(A,B)
    return np.array(background)

def airPLS(x, dssn_th, lambda_, porder, itermax):
    '''
    Adaptive iteratively reweighted penalized least squares for baseline fitting
    
    input
        x: input data (i.e. chromatogram or spectrum)
        lambda_: parameter that can be adjusted by user. The larger lambda is, the smoother the resulting background, z
        porder: adaptive iteratively reweighted penalized least squares for baseline fitting
    
    output
        the fitted background vector
    '''
    # ãƒã‚¤ãƒŠã‚¹å€¤ãŒã‚ã‚‹å ´åˆã®å‡¦ç†
    min_value = np.min(x)
    offset = 0
    if min_value < 0:
        offset = abs(min_value) + 1  # æœ€å°å€¤ã‚’1ã«ã™ã‚‹ãŸã‚ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        x = x + offset  # å…¨ä½“ã‚’ã‚·ãƒ•ãƒˆ
    
    m = x.shape[0]
    w = np.ones(m, dtype=np.float64)  # æ˜ç¤ºçš„ã«å‹ã‚’æŒ‡å®š
    x = np.asarray(x, dtype=np.float64)  # xã‚‚æ˜ç¤ºçš„ã«å‹ã‚’æŒ‡å®š
    
    for i in range(1, itermax + 1):
        z = WhittakerSmooth(x, w, lambda_, porder)
        d = x - z
        dssn = np.abs(d[d < 0].sum())
        
        # dssn ãŒã‚¼ãƒ­ã¾ãŸã¯éå¸¸ã«å°ã•ã„å ´åˆã‚’å›é¿
        if dssn < 1e-10:
            dssn = 1e-10
        
        # åæŸåˆ¤å®š
        if (dssn < dssn_th * (np.abs(x)).sum()) or (i == itermax):
            if i == itermax:
                print('WARNING: max iteration reached!')
            break
        
        # é‡ã¿ã®æ›´æ–°
        w[d >= 0] = 0  # d > 0 ã¯ãƒ”ãƒ¼ã‚¯ã®ä¸€éƒ¨ã¨ã—ã¦é‡ã¿ã‚’ç„¡è¦–
        w[d < 0] = np.exp(i * np.abs(d[d < 0]) / dssn)
        
        # å¢ƒç•Œæ¡ä»¶ã®èª¿æ•´
        if d[d < 0].size > 0:
            w[0] = np.exp(i * np.abs(d[d < 0]).max() / dssn)
        else:
            w[0] = 1.0  # é©åˆ‡ãªåˆæœŸå€¤
        
        w[-1] = w[0]

    return z

def detect_file_type(data):
    """
    Determine the structure of the input data.
    """
    try:
        if data.columns[0].split(':')[0] == "# Laser Wavelength":
            return "ramaneye_new"
        elif data.columns[0] == "WaveNumber":
            return "ramaneye_old"
        elif data.columns[0] == "Pixels":
            return "eagle"
        elif data.columns[0] == "ENLIGHTEN Version":
            return "wasatch"
        return "unknown"
    except:
        return "unknown"

def read_csv_file(uploaded_file, file_extension):
    """
    Read a CSV or TXT file into a DataFrame based on file extension.
    """
    try:
        uploaded_file.seek(0)
        if file_extension == "csv":
            data = pd.read_csv(uploaded_file, sep=',', header=0, index_col=None, on_bad_lines='skip')
        else:
            data = pd.read_csv(uploaded_file, sep='\t', header=0, index_col=None, on_bad_lines='skip')
        return data
    except UnicodeDecodeError:
        uploaded_file.seek(0)
        if file_extension == "csv":
            data = pd.read_csv(uploaded_file, sep=',', encoding='shift_jis', header=0, index_col=None, on_bad_lines='skip')
        else:
            data = pd.read_csv(uploaded_file, sep='\t', encoding='shift_jis', header=0, index_col=None, on_bad_lines='skip')
        return data
    except Exception as e:
        st.error(f"Error reading file: {e}")
        return None

def calculate_peak_width(spectrum, peak_idx, wavenum):
    """
    ãƒ”ãƒ¼ã‚¯ã®åŠå€¤å¹…ï¼ˆFWHMï¼‰ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    
    Parameters:
    spectrum (ndarray): ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿
    peak_idx (int): ãƒ”ãƒ¼ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    wavenum (ndarray): æ³¢æ•°ãƒ‡ãƒ¼ã‚¿
    
    Returns:
    fwhm (float): åŠå€¤å¹… (cmâ»Â¹)
    """
    if peak_idx <= 0 or peak_idx >= len(spectrum) - 1:
        return 0.0
    
    peak_intensity = spectrum[peak_idx]
    half_max = peak_intensity / 2.0
    
    # ãƒ”ãƒ¼ã‚¯ã‹ã‚‰å·¦å´ã«å‘ã‹ã£ã¦åŠå€¤ç‚¹ã‚’æ¢ã™
    left_idx = peak_idx
    while left_idx > 0 and spectrum[left_idx] > half_max:
        left_idx -= 1
    
    # ç·šå½¢è£œé–“ã§æ­£ç¢ºãªåŠå€¤ç‚¹ã‚’æ±‚ã‚ã‚‹
    if left_idx < peak_idx and spectrum[left_idx] <= half_max < spectrum[left_idx + 1]:
        # ç·šå½¢è£œé–“
        ratio = (half_max - spectrum[left_idx]) / (spectrum[left_idx + 1] - spectrum[left_idx])
        left_wavenum = wavenum[left_idx] + ratio * (wavenum[left_idx + 1] - wavenum[left_idx])
    else:
        left_wavenum = wavenum[left_idx] if left_idx >= 0 else wavenum[0]
    
    # ãƒ”ãƒ¼ã‚¯ã‹ã‚‰å³å´ã«å‘ã‹ã£ã¦åŠå€¤ç‚¹ã‚’æ¢ã™
    right_idx = peak_idx
    while right_idx < len(spectrum) - 1 and spectrum[right_idx] > half_max:
        right_idx += 1
    
    # ç·šå½¢è£œé–“ã§æ­£ç¢ºãªåŠå€¤ç‚¹ã‚’æ±‚ã‚ã‚‹
    if right_idx > peak_idx and spectrum[right_idx] <= half_max < spectrum[right_idx - 1]:
        # ç·šå½¢è£œé–“
        ratio = (half_max - spectrum[right_idx]) / (spectrum[right_idx - 1] - spectrum[right_idx])
        right_wavenum = wavenum[right_idx] + ratio * (wavenum[right_idx - 1] - wavenum[right_idx])
    else:
        right_wavenum = wavenum[right_idx] if right_idx < len(wavenum) else wavenum[-1]
    
    # åŠå€¤å¹…ã‚’è¨ˆç®—
    fwhm = abs(right_wavenum - left_wavenum)
    return fwhm

def remove_outliers_and_interpolate(spectrum, window_size=10, threshold_factor=3):
    """
    ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ã‚¯ï¼ˆå¤–ã‚Œå€¤ï¼‰ã‚’æ¤œå‡ºã—ã€è£œå®Œã™ã‚‹é–¢æ•°
    ã‚¹ãƒ‘ã‚¤ã‚¯ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®æ¨™æº–åå·®ãŒä¸€å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹å ´åˆã«æ¤œå‡ºã•ã‚Œã‚‹
    
    input:
        spectrum: numpy array, ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«
        window_size: ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯20ï¼‰
        threshold_factor: æ¨™æº–åå·®ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯5å€ï¼‰
    
    output:
        cleaned_spectrum: numpy array, ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å–ã‚Šé™¤ãè£œå®Œã—ãŸã‚¹ãƒšã‚¯ãƒˆãƒ«
    """
    spectrum_len = len(spectrum)
    cleaned_spectrum = spectrum.copy()
    
    for i in range(spectrum_len):
        # ç«¯ç‚¹ã§ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒè¶³ã‚Šãªã„ã®ã§ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’èª¿æ•´
        left_idx = max(i - window_size, 0)
        right_idx = min(i + window_size + 1, spectrum_len)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        window = spectrum[left_idx:right_idx]
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®ä¸­å¤®å€¤ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
        window_median = np.median(window)
        window_std = np.std(window)
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®å€¤ãŒæ¨™æº–åå·®ã®é–¾å€¤ã‚’è¶…ãˆã‚‹ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¤œå‡º
        if abs(spectrum[i] - window_median) > threshold_factor * window_std:
            # ã‚¹ãƒ‘ã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®å€¤ã‚’ä¸¡éš£ã®ä¸­å¤®å€¤ã§è£œå®Œ
            if i > 0 and i < spectrum_len - 1:  # ä¸¡éš£ã®å€¤ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                cleaned_spectrum[i] = (spectrum[i - 1] + spectrum[i + 1]) / 2
            elif i == 0:  # å·¦ç«¯ã®å ´åˆ
                cleaned_spectrum[i] = spectrum[i + 1]
            elif i == spectrum_len - 1:  # å³ç«¯ã®å ´åˆ
                cleaned_spectrum[i] = spectrum[i - 1] 
    return cleaned_spectrum
    
# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒé–¢æ•°
def optimize_thresholds_via_gridsearch(
    wavenum, spectrum, second_derivative,
    manual_add_peaks, manual_exclude_indices,
    current_prom_thres, current_deriv_thres,
    detected_original_peaks=None,
    resolution=40
):
    best_score = -np.inf
    best_prom_thres = current_prom_thres
    best_deriv_thres = current_deriv_thres

    # å€åŠåˆ†ã®ç¯„å›²ã§ãƒ­ã‚°ã‚¹ã‚±ãƒ¼ãƒ«æ¤œç´¢
    prom_range = np.linspace(current_prom_thres / 2, current_prom_thres * 2, resolution)
    deriv_range = np.linspace(current_deriv_thres / 2, current_deriv_thres * 2, resolution)

    for prom_thres in prom_range:
        for deriv_thres in deriv_range:
            peaks, _ = find_peaks(-second_derivative, height=deriv_thres)
            prominences = peak_prominences(-second_derivative, peaks)[0]
            mask = prominences > prom_thres
            final_peaks = set(peaks[mask])

            score = 0

            # æ‰‹å‹•è¿½åŠ ãŒå«ã¾ã‚Œã¦ã„ã‚Œã° +1
            for x, _ in manual_add_peaks:
                idx = np.argmin(np.abs(wavenum - x))
                if idx in final_peaks:
                    score += 1

            # é™¤å¤–å¯¾è±¡ãŒå«ã¾ã‚Œã¦ã„ã‚Œã° -1
            for idx in manual_exclude_indices:
                if idx in final_peaks:
                    score -= 1

            # è‡ªå‹•ãƒ”ãƒ¼ã‚¯ã®é€¸è„±ãƒšãƒŠãƒ«ãƒ†ã‚£
            if len(detected_original_peaks) > 0:
                for idx in final_peaks:
                    if idx not in detected_original_peaks:
                        score -= 1  # æ–°ãŸã«ç¾ã‚ŒãŸä½™åˆ†ãªãƒ”ãƒ¼ã‚¯
                for idx in detected_original_peaks:
                    if idx not in final_peaks:
                        score -= 1  # å…ƒã®ãƒ”ãƒ¼ã‚¯ãŒæ¶ˆãˆãŸ

            # ã‚¹ã‚³ã‚¢æœ€å¤§ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
            if score > best_score:
                best_score = score
                best_prom_thres = prom_thres
                best_deriv_thres = deriv_thres

    return {
        "prominence_threshold": best_prom_thres,
        "second_deriv_threshold": best_deriv_thres,
        "score": best_score
    }

def spectrum_analysis_mode():
    st.header("ğŸ“Š ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ")
    
   # --- LLM/RAGè¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
    st.sidebar.subheader("ğŸ¤– AIè§£æè¨­å®š")
    
    # âœ… è¿½åŠ ï¼ˆAIæœ‰åŠ¹/ç„¡åŠ¹åˆ‡ã‚Šæ›¿ãˆï¼‰
    enable_ai = st.sidebar.checkbox(
        "ğŸ§  AIæ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
        value=True,
        help="AIã«ã‚ˆã‚‹è‡ªå‹•æˆåˆ†æ¨å®šã¨è€ƒå¯Ÿã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
    )
    
    # RAGæ©Ÿèƒ½ã®æœ‰åŠ¹/ç„¡åŠ¹ã‚’é¸æŠ
    enable_rag = st.sidebar.checkbox(
        "ğŸ“š RAGæ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹",
        value=True,
        help="è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±æ¤œç´¢æ©Ÿèƒ½ã€‚ç„¡åŠ¹ã«ã™ã‚‹ã¨è»½é‡åŒ–ã•ã‚Œã¾ã™ã€‚"
    )
    
    # Mistralãƒ¢ãƒ‡ãƒ«é¸æŠ
    model_options = [
        "cl-tohoku/bert-base-japanese",
        "microsoft/DialoGPT-medium",  # ã‚ˆã‚Šè»½é‡ãªä»£æ›¿ãƒ¢ãƒ‡ãƒ«
    ]
    
    selected_model = st.sidebar.selectbox(
        "ğŸ§  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
        model_options,
        index=0,
        help="ä½¿ç”¨ã™ã‚‹Hugging Faceãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚ä¸Šä½ã»ã©è»½é‡ã§ã™ã€‚"
    )
    
    # RAGæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¡¨ç¤º
    uploaded_files = []
    if enable_rag:
        uploaded_files = st.sidebar.file_uploader(
            "ğŸ“„ æ–‡çŒ®PDFã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
            type=["pdf"],
            accept_multiple_files=True
        )
    
    # ä¸€æ™‚ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    TEMP_DIR = "./tmp_uploads"
    os.makedirs(TEMP_DIR, exist_ok=True)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
    if enable_rag:
        if 'rag_system' not in st.session_state:
            st.session_state.rag_system = SimpleRAGSystem()
            st.session_state.rag_db_built = False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ§‹ç¯‰
        if st.sidebar.button("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"):
            if not uploaded_files:
                st.sidebar.warning("æ–‡çŒ®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("è«–æ–‡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­..."):
                    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
                    for uploaded_file in uploaded_files:
                        save_path = os.path.join(TEMP_DIR, uploaded_file.name)
                        with open(save_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())

                    # ä¿å­˜ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ã£ã¦ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰
                    st.session_state.rag_system.build_vector_database(TEMP_DIR)
                    st.session_state.rag_db_built = True
                    st.sidebar.success(f"âœ… {len(uploaded_files)} ä»¶ã®PDFã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹è¡¨ç¤º
        if st.session_state.rag_db_built:
            st.sidebar.success("âœ… è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰æ¸ˆã¿")
        else:
            st.sidebar.info("â„¹ï¸ è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœªæ§‹ç¯‰")
    else:
        # RAGç„¡åŠ¹æ™‚ã®çŠ¶æ…‹åˆæœŸåŒ–
        if 'rag_system' in st.session_state:
            del st.session_state.rag_system
        st.session_state.rag_db_built = False
        st.sidebar.info("ğŸ’¨ è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆRAGæ©Ÿèƒ½ç„¡åŠ¹ï¼‰")
    
    # LLMã®åˆæœŸåŒ–ï¼ˆAIæ©Ÿèƒ½æœ‰åŠ¹æ™‚ã®ã¿ï¼‰
    if enable_ai:
        if 'simple_llm' not in st.session_state:
            st.session_state.simple_llm = None
            st.session_state.current_model = None
            
        # ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ã®å†åˆæœŸåŒ–
        if st.session_state.current_model != selected_model:
            st.session_state.simple_llm = None
            st.session_state.current_model = selected_model
        
        st.sidebar.success("ğŸ¤– AIè§£ææ©Ÿèƒ½æœ‰åŠ¹")
    else:
        # AIç„¡åŠ¹æ™‚ã®çŠ¶æ…‹åˆæœŸåŒ–
        if 'simple_llm' in st.session_state:
            del st.session_state.simple_llm
        st.session_state.current_model = None
        st.sidebar.info("ğŸ’¨ è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼ˆAIæ©Ÿèƒ½ç„¡åŠ¹ï¼‰")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è£œè¶³æŒ‡ç¤ºæ¬„ã‚’è¿½åŠ 
    user_hint = st.sidebar.text_area(
        "ğŸ§ª AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰",
        placeholder="ä¾‹ï¼šã“ã®è©¦æ–™ã¯ãƒãƒªã‚¨ãƒãƒ¬ãƒ³ç³»é«˜åˆ†å­ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€ãªã©"
    )
    
    st.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ")

    # --- äº‹å‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---
    pre_start_wavenum = 400
    pre_end_wavenum = 2000
    
    # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–ï¼ˆUIè¡¨ç¤ºã‚ˆã‚Šã‚‚å‰ï¼ï¼‰ ---
    for key, default in {
        "prominence_threshold": 100,
        "second_deriv_threshold": 100,
        "savgol_wsize": 5,
        "spectrum_type_select": "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤",
        "second_deriv_smooth": 5,
        "manual_peak_keys": []
    }.items():
        if key not in st.session_state:
            st.session_state[key] = default
    # --- UIãƒ‘ãƒãƒ«ï¼ˆSidebarï¼‰ ---
    start_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆé–‹å§‹ï¼‰:", -200, 4800, value=pre_start_wavenum, step=100)
    end_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆçµ‚äº†ï¼‰:", -200, 4800, value=pre_end_wavenum, step=100)
    dssn_th = st.sidebar.number_input("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼:", 1, 10000, value=1000, step=1) / 1e7
    
    savgol_wsize = st.sidebar.number_input(
        "ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º:", 3, 101, 
        value=st.session_state["savgol_wsize"], step=2, key="savgol_wsize"
    )
    
    st.sidebar.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºè¨­å®š")
    
    spectrum_type = st.sidebar.selectbox(
        "è§£æã‚¹ãƒšã‚¯ãƒˆãƒ«:", ["ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤", "ç§»å‹•å¹³å‡å¾Œ"], 
        index=0, key="spectrum_type_select"
    )
    
    second_deriv_smooth = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–:", 3, 35,
        value=st.session_state["second_deriv_smooth"],
        step=2, key="second_deriv_smooth"
    )
    
    # ä¸€æ™‚çš„ãªã‚»ãƒƒã‚·ãƒ§ãƒ³å¤‰æ•°ãŒå­˜åœ¨ã™ã‚‹ãªã‚‰ãã¡ã‚‰ã‚’ä½¿ã†ï¼ˆãªã‘ã‚Œã°é€šå¸¸ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å€¤ï¼‰
    prom_default = float(st.session_state.get("prominence_threshold_temp", st.session_state.get("prominence_threshold", 100.0)))
    second_default = float(st.session_state.get("second_deriv_threshold_temp", st.session_state.get("second_deriv_threshold", 100.0)))
    
    second_deriv_threshold = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†é–¾å€¤:",
        min_value=0.0,
        max_value=1000.0,
        value=second_default,
        step=10.0,
        key="second_deriv_threshold"
    )
    
    peak_prominence_threshold = st.sidebar.number_input(
        "ãƒ”ãƒ¼ã‚¯Prominenceé–¾å€¤:",
        min_value=0.0,
        max_value=1000.0,
        value=prom_default,
        step=10.0,
        key="prominence_threshold"
    )

    # --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆ1ç®‡æ‰€ã«çµ±ä¸€ï¼‰ ---
    uploaded_spectrum_files = st.file_uploader("ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", accept_multiple_files=True, key="spectrum_file_uploader")
    
    # --- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œå‡º ---
    new_filenames = [f.name for f in uploaded_spectrum_files] if uploaded_spectrum_files else []
    prev_filenames = st.session_state.get("uploaded_filenames", [])

    # --- è¨­å®šå¤‰æ›´æ¤œå‡º ---
    config_keys = ["spectrum_type_select", "second_deriv_smooth", "second_deriv_threshold", "prominence_threshold"]
    config_changed = any(
        st.session_state.get(f"prev_{key}") != st.session_state[key] for key in config_keys
    )
    file_changed = new_filenames != prev_filenames

    # --- æ‰‹å‹•ãƒ”ãƒ¼ã‚¯åˆæœŸåŒ–æ¡ä»¶ ---
    if config_changed or file_changed:
        for key in list(st.session_state.keys()):
            if key.endswith("_manual_peaks"):
                del st.session_state[key]
        st.session_state["manual_peak_keys"] = []
        st.session_state["uploaded_filenames"] = new_filenames
        for k in config_keys:
            st.session_state[f"prev_{k}"] = st.session_state[k]
            
    file_labels = []
    all_spectra = []
    all_bsremoval_spectra = []
    all_averemoval_spectra = []
    all_wavenum = []
    
    if uploaded_spectrum_files:
        config_keys = [
            "spectrum_type_select",
            "second_deriv_smooth",
            "second_deriv_threshold",
            "prominence_threshold"
        ]
        # ã‚»ãƒ¼ãƒ•ãªä»£å…¥å‡¦ç†ï¼ˆKeyErroré˜²æ­¢ï¼‰
        for k in config_keys:
            st.session_state[f"prev_{k}"] = st.session_state.get(k)

        
        # --- ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œå‡º ---
        file_changed = new_filenames != prev_filenames
        
        # --- æ‰‹å‹•ãƒ”ãƒ¼ã‚¯åˆæœŸåŒ–æ¡ä»¶ ---
        if config_changed or file_changed:
            for key in list(st.session_state.keys()):
                if key.endswith("_manual_peaks"):
                    del st.session_state[key]
            st.session_state["manual_peak_keys"] = []
            st.session_state["uploaded_filenames"] = new_filenames
        
            # å®‰å…¨ã« prev_ ã‚’ã‚»ãƒƒãƒˆ
            for k in config_keys:
                st.session_state[f"prev_{k}"] = st.session_state.get(k)
                
        for uploaded_file in uploaded_spectrum_files:
            file_name = uploaded_file.name
            file_extension = file_name.split('.')[-1] if '.' in file_name else ''

            try:
                data = read_csv_file(uploaded_file, file_extension)
                file_type = detect_file_type(data)
                uploaded_file.seek(0)
                if file_type == "unknown":
                    st.error(f"{file_name}ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥ã§ãã¾ã›ã‚“ã€‚")
                    continue

                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¯¾ã™ã‚‹å‡¦ç†
                if file_type == "wasatch":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Wasatch ENLIGHTEN - {file_name}")
                    lambda_ex = 785
                    data = pd.read_csv(uploaded_file, encoding='shift-jis', skiprows=46)
                    pre_wavelength = np.array(data["Wavelength"].values)
                    pre_wavenum = (1e7 / lambda_ex) - (1e7 / pre_wavelength)
                    pre_spectra = np.array(data["Processed"].values)

                elif file_type == "ramaneye_old":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data - {file_name}")
                    pre_wavenum = data["WaveNumber"]
                    pre_spectra = np.array(data.iloc[:, -1])
                    if pre_wavenum.iloc[0] > pre_wavenum.iloc[1]:
                        # pre_wavenum ã¨ pre_spectra ã‚’åè»¢
                        pre_wavenum = pre_wavenum[::-1]
                        pre_spectra = pre_spectra[::-1]
                        
                elif file_type == "ramaneye_new":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: RamanEye Data - {file_name}")
                    
                    data = pd.read_csv(uploaded_file, skiprows=9)
                    pre_wavenum = data["WaveNumber"]
                    pre_spectra = np.array(data.iloc[:, -1])

                    if pre_wavenum.iloc[0] > pre_wavenum.iloc[1]:
                        # pre_wavenum ã¨ pre_spectra ã‚’åè»¢
                        pre_wavenum = pre_wavenum[::-1]
                        pre_spectra = pre_spectra[::-1]
                        
                elif file_type == "eagle":
                    st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: Eagle Data - {file_name}")
                    data_transposed = data.transpose()
                    header = data_transposed.iloc[:3]  # æœ€åˆã®3è¡Œ
                    reversed_data = data_transposed.iloc[3:].iloc[::-1]
                    data_transposed = pd.concat([header, reversed_data], ignore_index=True)
                    pre_wavenum = np.array(data_transposed.iloc[3:, 0])
                    pre_spectra = np.array(data_transposed.iloc[3:, 1])
                
                start_index = find_index(pre_wavenum, start_wavenum)
                end_index = find_index(pre_wavenum, end_wavenum)

                wavenum = np.array(pre_wavenum[start_index:end_index+1])
                spectra = np.array(pre_spectra[start_index:end_index+1])

                # Baseline and spike removal 
                spectra_spikerm = remove_outliers_and_interpolate(spectra)
                mveAve_spectra = signal.medfilt(spectra_spikerm, savgol_wsize)
                lambda_ = 10e2
                baseline = airPLS(mveAve_spectra, dssn_th, lambda_, 2, 30)
                BSremoval_specta = spectra_spikerm - baseline
                BSremoval_specta_pos = BSremoval_specta + abs(np.minimum(spectra_spikerm, 0))  # è² å€¤ã‚’è£œæ­£

                # ç§»å‹•å¹³å‡å¾Œã®ã‚¹ãƒšã‚¯ãƒˆãƒ«
                Averemoval_specta = mveAve_spectra  - baseline
                Averemoval_specta_pos = Averemoval_specta + abs(np.minimum(mveAve_spectra, 0))  # è² å€¤ã‚’è£œæ­£

                # å„ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’æ ¼ç´
                file_labels.append(file_name)  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
                all_wavenum.append(wavenum)
                all_spectra.append(spectra)
                all_bsremoval_spectra.append(BSremoval_specta_pos)
                all_averemoval_spectra.append(Averemoval_specta_pos)
                
            except Exception as e:
                st.error(f"{file_name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®å®Ÿè¡Œ
        if 'peak_detection_triggered' not in st.session_state:
            st.session_state['peak_detection_triggered'] = False
    
        # ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã‚‰ãƒˆãƒªã‚¬ãƒ¼ã‚’ONã«
        if st.button("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œ"):
            st.session_state['peak_detection_triggered'] = True
        
        if st.session_state['peak_detection_triggered']:
            st.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ")
            
            peak_results = []
            
            # ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º
            st.info(f"""
            **æ¤œå‡ºè¨­å®š:**
            - ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—: {spectrum_type}
            - 2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–: {second_deriv_smooth}, é–¾å€¤: {second_deriv_threshold} (ãƒ”ãƒ¼ã‚¯æ¤œå‡ºç”¨)
            - ãƒ”ãƒ¼ã‚¯å“ç«‹åº¦é–¾å€¤: {peak_prominence_threshold}
            """)
            
            for i, file_name in enumerate(file_labels):
                # é¸æŠã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ
                if spectrum_type == "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤":
                    selected_spectrum = all_bsremoval_spectra[i]
                else:  # ç§»å‹•å¹³å‡å¾Œ
                    selected_spectrum = all_averemoval_spectra[i]
                
                wavenum = all_wavenum[i]
                
                # 2æ¬¡å¾®åˆ†è¨ˆç®—ï¼ˆãƒ”ãƒ¼ã‚¯æ¤œå‡ºç”¨ï¼‰
                if len(selected_spectrum) > second_deriv_smooth:
                    second_derivative = savgol_filter(selected_spectrum, second_deriv_smooth, 2, deriv=2)
                else:
                    second_derivative = np.gradient(np.gradient(selected_spectrum))
                
                # 2æ¬¡å¾®åˆ†ã®ã¿ã«ã‚ˆã‚‹ãƒ”ãƒ¼ã‚¯æ¤œå‡ºï¼ˆprominenceåˆ¤å®šä»˜ã, propertiesã¯ãƒ€ãƒŸãƒ¼ï¼‰
                peaks, properties = find_peaks(-second_derivative, height=second_deriv_threshold)
                all_peaks, properties = find_peaks(-second_derivative)

                if len(peaks) > 0:
                    # Peak prominences ã‚’è¨ˆç®—
                    prominences = peak_prominences(-second_derivative, peaks)[0]
                    all_prominences = peak_prominences(-second_derivative, all_peaks)[0]

                    # Prominence é–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    mask = prominences > peak_prominence_threshold
                    filtered_peaks = peaks[mask]
                    filtered_prominences = prominences[mask]
                    
                    # Â±2ãƒ”ã‚¯ã‚»ãƒ«ç¯„å›²å†…ã§å±€æ‰€æ¥µå¤§ã‚’å†ç¢ºèªãƒ»è£œæ­£
                    corrected_peaks = []
                    corrected_prominences = []
                    
                    for peak_idx, prom in zip(filtered_peaks, filtered_prominences):
                        # Â±2ã®ç¯„å›²å†…ã§æœ€å¤§å€¤ã‚’æ¢ã™ï¼ˆç¯„å›²è¶…ãˆãªã„ã‚ˆã†åˆ¶é™ï¼‰
                        window_start = max(0, peak_idx - 2)
                        window_end = min(len(selected_spectrum), peak_idx + 3)
                        local_window = selected_spectrum[window_start:window_end]
                        
                        # å±€æ‰€æœ€å¤§å€¤ã®ä½ç½®ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
                        local_max_idx = np.argmax(local_window)
                        corrected_idx = window_start + local_max_idx
                    
                        corrected_peaks.append(corrected_idx)
                        
                        # Prominence ã‚‚å†è¨ˆç®—
                        local_prom = peak_prominences(-second_derivative, [corrected_idx])[0][0]
                        corrected_prominences.append(local_prom)
                    
                    # numpyé…åˆ—ã«å¤‰æ›
                    filtered_peaks = np.array(corrected_peaks)
                    filtered_prominences = np.array(corrected_prominences)
                else:
                    filtered_peaks = np.array([])
                    filtered_prominences = np.array([])
                
                # çµæœã‚’ä¿å­˜
                peak_data = {
                    'file_name': file_name,
                    'detected_peaks': filtered_peaks,
                    'detected_prominences': filtered_prominences,
                    'wavenum': wavenum,
                    'spectrum': selected_spectrum,
                    'second_derivative': second_derivative,
                    'second_deriv_smooth': second_deriv_smooth,
                    'second_deriv_threshold': second_deriv_threshold,
                    'prominence_threshold': peak_prominence_threshold,
                    'all_peaks': all_peaks,
                    'all_prominences': all_prominences,
                }
                peak_results.append(peak_data)
                
                # çµæœã‚’è¡¨ç¤º
                st.write(f"**{file_name}**")
                st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æ•°: {len(filtered_peaks)} (2æ¬¡å¾®åˆ† + prominenceåˆ¤å®š)")
                
                # ãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
                if len(filtered_peaks) > 0:
                    peak_wavenums = wavenum[filtered_peaks]
                    peak_intensities = selected_spectrum[filtered_peaks]
                    st.write("**æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯:**")
                    peak_table = pd.DataFrame({
                        'ãƒ”ãƒ¼ã‚¯ç•ªå·': range(1, len(peak_wavenums) + 1),
                        'æ³¢æ•° (cmâ»Â¹)': [f"{wn:.1f}" for wn in peak_wavenums],
                        'å¼·åº¦': [f"{intensity:.3f}" for intensity in peak_intensities],
                        'Prominence': [f"{prom:.4f}" for prom in filtered_prominences]
                    })
                    st.table(peak_table)
                else:
                    st.write("ãƒ”ãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
                for result in peak_results:
                    file_key = result['file_name']
                    
                    filtered_peaks = result['detected_peaks']
                    filtered_prominences = result['detected_prominences']
                
                    fig = make_subplots(
                        rows=3, cols=1,
                        shared_xaxes=True,
                        subplot_titles=[
                            f'{file_key} - {spectrum_type}',
                            f'{file_key} - å¾®åˆ†ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¯”è¼ƒ',
                            f'{file_key} - Prominence vs æ³¢æ•°'
                        ],
                        vertical_spacing=0.07,
                        row_heights=[0.4, 0.3, 0.3]
                    )
                
                    # ä¸Šæ®µ: ã‚¹ãƒšã‚¯ãƒˆãƒ«è¡¨ç¤º
                    fig.add_trace(
                        go.Scatter(
                            x=result['wavenum'],
                            y=result['spectrum'],
                            mode='lines',
                            name=spectrum_type,
                            line=dict(color='blue', width=1)
                        ),
                        row=1, col=1
                    )
                
                    if len(filtered_peaks) > 0:
                        fig.add_trace(
                            go.Scatter(
                                x=result['wavenum'][filtered_peaks],
                                y=result['spectrum'][filtered_peaks],
                                mode='markers',
                                name='æ¤œå‡ºãƒ”ãƒ¼ã‚¯ï¼ˆæœ‰åŠ¹ï¼‰',
                                marker=dict(color='red', size=8, symbol='circle')
                            ),
                            row=1, col=1
                        )
                
                    # ä¸­æ®µ: 2æ¬¡å¾®åˆ†è¡¨ç¤º
                    fig.add_trace(
                        go.Scatter(
                            x=result['wavenum'],
                            y=result['second_derivative'],
                            mode='lines',
                            name='2æ¬¡å¾®åˆ†',
                            line=dict(color='purple', width=1)
                        ),
                        row=2, col=1
                    )
                
                    fig.add_hline(y=0, line_dash="dash", line_color="black", opacity=0.5, row=2, col=1)
                
                    # ä¸‹æ®µ: Prominenceãƒ—ãƒ­ãƒƒãƒˆ
                    fig.add_trace(
                        go.Scatter(
                            x=result['wavenum'][result['all_peaks']],
                            y=result['all_prominences'],
                            mode='markers',
                            name='å…¨ãƒ”ãƒ¼ã‚¯ã®Prominence',
                            marker=dict(color='orange', size=4)
                        ),
                        row=3, col=1
                    )
                
                    if len(filtered_peaks) > 0:
                        fig.add_trace(
                            go.Scatter(
                                x=result['wavenum'][filtered_peaks],
                                y=filtered_prominences,
                                mode='markers',
                                name='æœ‰åŠ¹ãªProminence',
                                marker=dict(color='red', size=7, symbol='circle')
                            ),
                            row=3, col=1
                        )
                
                    fig.update_layout(height=800, margin=dict(t=80, b=40))
                    fig.update_xaxes(title_text="æ³¢æ•° (cmâ»Â¹)", row=3, col=1)
                    fig.update_yaxes(title_text="å¼·åº¦", row=1, col=1)
                    fig.update_yaxes(title_text="å¾®åˆ†å€¤", row=2, col=1)
                
                    # âœ… Cloudäº’æ›ã®ãŸã‚ st.plotly_chart ã‚’ä½¿ç”¨
                    st.plotly_chart(fig, use_container_width=True)
                
                # AIè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³ - ãƒ”ãƒ¼ã‚¯ç¢ºå®šå¾Œã®è€ƒå¯Ÿæ©Ÿèƒ½
                st.markdown("---")
                st.subheader(f"ğŸ¤– AIè§£æ - {file_key}")
                
                # æœ€çµ‚çš„ãªãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’åé›†ï¼ˆè‡ªå‹•æ¤œå‡º + æ‰‹å‹•è¿½åŠ  - é™¤å¤–ï¼‰
                final_peak_data = []
                
                # æœ‰åŠ¹ãªè‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯
                for idx, prom in zip(filtered_peaks, filtered_prominences):
                    final_peak_data.append({
                        'wavenumber': result['wavenum'][idx],
                        'intensity': result['spectrum'][idx],
                        'prominence': prom,
                        'type': 'auto'
                    })
                
                # æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯
                for x, y in st.session_state[f"{file_key}_manual_peaks"]:
                    idx = np.argmin(np.abs(result['wavenum'] - x))
                    try:
                        prom = peak_prominences(-result['second_derivative'], [idx])[0][0]
                    except:
                        prom = 0.0
                    
                    final_peak_data.append({
                        'wavenumber': x,
                        'intensity': y,
                        'prominence': prom,
                        'type': 'manual'
                    })
                
                if final_peak_data:
                    st.write(f"**æœ€çµ‚ç¢ºå®šãƒ”ãƒ¼ã‚¯æ•°: {len(final_peak_data)}**")
                    
                    # ãƒ”ãƒ¼ã‚¯è¡¨ç¤º
                    peak_summary_df = pd.DataFrame([
                        {
                            'ãƒ”ãƒ¼ã‚¯ç•ªå·': i+1,
                            'æ³¢æ•° (cmâ»Â¹)': f"{peak['wavenumber']:.1f}",
                            'å¼·åº¦': f"{peak['intensity']:.3f}",
                            'Prominence': f"{peak['prominence']:.3f}",
                            'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak['type'] == 'auto' else 'æ‰‹å‹•è¿½åŠ '
                        }
                        for i, peak in enumerate(final_peak_data)
                    ])
                    
                    # åŸºæœ¬è§£ææƒ…å ±ã®è¡¨ç¤ºï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
                    st.info("ğŸ”¬ åŸºæœ¬è§£ææƒ…å ±")
                    st.write("æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„è§£é‡ˆï¼š")
                    
                    # åŸºæœ¬çš„ãªãƒ”ãƒ¼ã‚¯è§£é‡ˆ
                    analyzer = RamanSpectrumAnalyzer()
                    basic_analysis = analyzer._generate_basic_analysis(final_peak_data)
                    st.markdown(basic_analysis)
                    
                    # åŸºæœ¬ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    basic_report = f"""ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«åŸºæœ¬è§£æãƒ¬ãƒãƒ¼ãƒˆ
ãƒ•ã‚¡ã‚¤ãƒ«å: {file_key}
è§£ææ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

=== æ¤œå‡ºãƒ”ãƒ¼ã‚¯æƒ…å ± ===
{peak_summary_df.to_string(index=False)}

=== åŸºæœ¬è§£æ ===
{basic_analysis}
"""
                    st.download_button(
                        label="ğŸ“„ åŸºæœ¬è§£æãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=basic_report,
                        file_name=f"raman_basic_report_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain",
                        key=f"download_basic_report_{file_key}"
                    )
                    
                    # AIè§£æå®Ÿè¡Œãƒœã‚¿ãƒ³ï¼ˆAIæ©Ÿèƒ½æœ‰åŠ¹æ™‚ã®ã¿è¡¨ç¤ºï¼‰
                    if enable_ai:
                        if st.button(f"ğŸ§  AIè§£æã‚’å®Ÿè¡Œ - {file_key}", key=f"ai_analysis_{file_key}"):
                            # LLMã®åˆæœŸåŒ–ï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
                            if st.session_state.simple_llm is None:
                                st.session_state.simple_llm = SimpleLLM(selected_model)
                            
                            with st.spinner("AIè¨€èªãƒ¢ãƒ‡ãƒ«ã§è§£æä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."):
                                analysis_report = None
                                start_time = time.time()
                        
                                try:
                                    # é–¢é€£æ–‡çŒ®ã‚’æ¤œç´¢ï¼ˆRAGæœ‰åŠ¹æ™‚ã®ã¿ï¼‰
                                    relevant_docs = []
                                    if enable_rag and hasattr(st.session_state, 'rag_system') and st.session_state.rag_db_built:
                                        search_terms = ' '.join([f"{p['wavenumber']:.0f}cm-1" for p in final_peak_data[:5]])
                                        search_query = f"ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ ãƒ”ãƒ¼ã‚¯ {search_terms}"
                                        relevant_docs = st.session_state.rag_system.search_relevant_documents(search_query, top_k=5)
                        
                                    # AIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
                                    analysis_prompt = analyzer.generate_analysis_prompt(
                                        peak_data=final_peak_data,
                                        relevant_docs=relevant_docs,
                                        user_hint=user_hint
                                    )
                                    
                                    # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ç”¨ã‚¨ãƒªã‚¢
                                    st.success("âœ… AIã®å¿œç­”ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºï¼‰")
                                    stream_area = st.empty()
                                    full_response = ""
                        
                                    # AIã«ã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã§å•ã„åˆã‚ã›
                                    for chunk in st.session_state.simple_llm.generate_stream_response(analysis_prompt, max_tokens=256):
                                        full_response += chunk
                                        stream_area.markdown(full_response)
                    
                                    # ãƒ”ãƒ¼ã‚¯æƒ…å ±ã¾ã¨ã‚è¡¨
                                    peak_summary_df = pd.DataFrame([
                                        {
                                            'ãƒ”ãƒ¼ã‚¯ç•ªå·': i + 1,
                                            'æ³¢æ•° (cmâ»Â¹)': f"{peak['wavenumber']:.1f}",
                                            'å¼·åº¦': f"{peak['intensity']:.3f}",
                                            'Prominence': f"{peak['prominence']:.3f}",
                                            'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak['type'] == 'auto' else 'æ‰‹å‹•è¿½åŠ '
                                        }
                                        for i, peak in enumerate(final_peak_data)
                                    ])
                        
                                    # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                                    analysis_report = f"""ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¬ãƒãƒ¼ãƒˆ
ãƒ•ã‚¡ã‚¤ãƒ«å: {file_key}
è§£ææ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model}

=== æ¤œå‡ºãƒ”ãƒ¼ã‚¯æƒ…å ± ===
{peak_summary_df.to_string(index=False)}

=== AIè§£æçµæœ ===
{full_response}

=== å‚ç…§æ–‡çŒ® ===
"""
                                    for i, doc in enumerate(relevant_docs, 1):
                                        analysis_report += f"{i}. {doc['metadata']['filename']}ï¼ˆé¡ä¼¼åº¦: {doc['similarity_score']:.3f}ï¼‰\n"
                        
                                    # å‡¦ç†æ™‚é–“ã®è¡¨ç¤º
                                    elapsed = time.time() - start_time
                                    st.info(f"ğŸ•’ è§£æã«ã‹ã‹ã£ãŸæ™‚é–“: {elapsed:.2f} ç§’")
                                    
                                    # è§£æçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                                    st.session_state[f"{file_key}_ai_analysis"] = {
                                        'analysis': full_response,
                                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                                        'model': selected_model,
                                        'peak_data': final_peak_data
                                    }
                        
                                    # ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
                                    st.download_button(
                                        label="ğŸ“„ AIè§£æãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=analysis_report,
                                        file_name=f"raman_ai_analysis_report_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                                        mime="text/plain",
                                        key=f"download_ai_report_{file_key}"
                                    )
                        
                                except Exception as e:
                                    st.error("AIè§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
                                    st.code(str(e))
                    
                    # éå»ã®è§£æçµæœè¡¨ç¤ºï¼ˆAIæ©Ÿèƒ½æœ‰åŠ¹æ™‚ã®ã¿ï¼‰
                    if enable_ai and f"{file_key}_ai_analysis" in st.session_state:
                        with st.expander("ğŸ“œ éå»ã®AIè§£æçµæœã‚’è¡¨ç¤º"):
                            past_analysis = st.session_state[f"{file_key}_ai_analysis"]
                            st.write(f"**è§£ææ—¥æ™‚:** {past_analysis['timestamp']}")
                            st.write(f"**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {past_analysis['model']}")
                            st.markdown("**è§£æçµæœ:**")
                            st.markdown(past_analysis['analysis'])
                
                else:
                    st.info("ç¢ºå®šã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€æ‰‹å‹•ã§ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
                
            # å…¨ãƒ”ãƒ¼ã‚¯çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã«ã™ã‚‹
            all_peaks_data = []
            for result in peak_results:
                # æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ï¼ˆ2æ¬¡å¾®åˆ† + prominenceåˆ¤å®šï¼‰
                if len(result['detected_peaks']) > 0:
                    peak_wavenums = result['wavenum'][result['detected_peaks']]
                    peak_intensities = result['spectrum'][result['detected_peaks']]
                    for j, (wn, intensity, prominence) in enumerate(zip(peak_wavenums, peak_intensities, result['detected_prominences'])):
                        all_peaks_data.append({
                            'ãƒ•ã‚¡ã‚¤ãƒ«å': result['file_name'],
                            'ãƒ”ãƒ¼ã‚¯ç•ªå·': j + 1,
                            'æ³¢æ•°_cm-1': f"{wn:.1f}",
                            'å¼·åº¦': f"{intensity:.6f}",
                            'Prominence': f"{prominence:.6f}",
                            'ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—': spectrum_type,
                            'æ¤œå‡ºæ–¹æ³•': '2æ¬¡å¾®åˆ†+prominence',
                            'å¹³æ»‘åŒ–æ•°å€¤': result['second_deriv_smooth'],
                            '2æ¬¡å¾®åˆ†é–¾å€¤': result['second_deriv_threshold'],
                            'Prominenceé–¾å€¤': result['prominence_threshold']
                        })
            
            if all_peaks_data:
                peaks_df = pd.DataFrame(all_peaks_data)
                csv = peaks_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"peak_detection_results_{spectrum_type}_prominence.csv",
                    mime="text/csv"
                )

def main():
    st.set_page_config(
        page_title="AIã«ã‚ˆã‚‹ãƒ©ãƒãƒ³ãƒ”ãƒ¼ã‚¯è§£æ", 
        page_icon="ğŸ“Š", 
        layout="wide"
    )
    
    st.title("ğŸ“Š AIã«ã‚ˆã‚‹ãƒ©ãƒãƒ³ãƒ”ãƒ¼ã‚¯è§£æ")
    st.markdown("---")
        
    st.sidebar.markdown("---")
    st.sidebar.header("ğŸ“‹ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    
    spectrum_analysis_mode()
    
    # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
    st.sidebar.markdown("---")
    st.sidebar.markdown("""
    ### ğŸ“š ä½¿ç”¨æ–¹æ³•
    1. **Mistralãƒ¢ãƒ‡ãƒ«é¸æŠ**: ä½¿ç”¨ã™ã‚‹Hugging Face Mistralãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    2. **è«–æ–‡ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: RAGæ©Ÿèƒ½ç”¨ã®è«–æ–‡PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    3. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰**: è«–æ–‡ã‹ã‚‰æ¤œç´¢ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
    4. **ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: è§£æã™ã‚‹ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    5. **ãƒ”ãƒ¼ã‚¯æ¤œå‡º**: è‡ªå‹•æ¤œå‡º + æ‰‹å‹•èª¿æ•´ã§ãƒ”ãƒ¼ã‚¯ã‚’ç¢ºå®š
    6. **AIè§£æå®Ÿè¡Œ**: ç¢ºå®šãƒ”ãƒ¼ã‚¯ã‚’åŸºã«MistralãŒè€ƒå¯Ÿã‚’ç”Ÿæˆ
    
    ### ğŸ”§ ã‚µãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
    - **ã‚¹ãƒšã‚¯ãƒˆãƒ«**: CSV, TXT (RamanEye, Wasatch, Eagleå¯¾å¿œ)
    - **è«–æ–‡**: PDF, DOCX, TXT
    
    ### âš ï¸ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶
    - **GPUæ¨å¥¨**: Mistralãƒ¢ãƒ‡ãƒ«ã¯é«˜é€ŸåŒ–ã®ãŸã‚GPUä½¿ç”¨ã‚’æ¨å¥¨
    - **ãƒ¡ãƒ¢ãƒª**: 8GBä»¥ä¸Šã®RAM/VRAMã‚’æ¨å¥¨
    - **ä¾å­˜é–¢ä¿‚**: requirements.txtã®å…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦
    """)

if __name__ == "__main__":
    main()
